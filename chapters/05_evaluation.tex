\chapter{Evaluation}\label{ch:eval}
roter faden: results of manual analysis vs benchmark (ie. is context better than no context)
take their method + analyze it on a qualitative + quantitative basis on a big android dataset

% http://libguides.usc.edu/writingguide/methodology

\todo[inline]{does this order make sense or rather split by research question?}
\section{Methodology}

\subsection{Qualitative Evaluation / Android Case Study}
android apps, blabla
manual review of top 50 findings for each ``method''
-> which are the different methods?

\subsection{Automated Benchmark}
degrading type usages + checking if they will be detected

sometimes in even further degraded settings (degrade more than one)

general idea behind evaluation method and what we are trying to detect
actually it's a sort of simulation (see recommender system book p. 301f) - micro vs macro evaluation, \ldots
    imitation of the real system of software development
    using a much simpler system, namely dropping method calls
    obvious question: how similar are the such created mmcs to mmcs in the wild?

Metrics explanation - Precision, Recall
    additional metrics from recommender systems book p.245f (robustness, learning rate - p. 261)
    performance -> training time + performance when working

\section{Results}
Which questions are we trying to answer with the evaluation?
quick overview + method to do this

qualitative evaluation: are the findings useful in practice?
quantitative evaluation: given some assumptions, how useful can we expect this technique to be? robustness? Qualitative vs automatic same results?
make sure I will be able to a) answer those questions and b) the answers are ``interesting''

große frage: bringt das Verfahren etwas?

for each question: explanation of question, then results + interprettation
\subsection{Dataset Overview}
what kind of dataset do we have? (how many apps, how big, how many TUs, how are the TUs split between the apps -> size of applications)
what are typeusages in Android apps like? (length of method list, percentage on Android framework, percentage on other stuff, etc) 

general data about avg tus related to different settings etc -> what does our dataset even consist of?

look into and understand the type of data i have -> how often small inputs, how often big, ...
-> paint a bunch of graphs of the pkl results! (histogram of tu list sizes,  etc) (correctly bin histogram!)
 tu method list sizes - also check their paper, q mas?

\subsection{RQ1: Does the Strangeness Score behave as expected on Android Apps?}

do the general assumptions hold? (most tus have a low score, most apps have few findings, etc)
    -> answer with graphs related to general score verteilung
    -> most tus are ``normal'', a few are ``abnormal''
    what do those assumptions mean? -> they expect some kind of uniformity to the type usages, probably mostly present in GUI etc frameworks
verteilung of strangeness scores, histogram of it

\subsection{RQ2: How ``useful'' is this technique?}
\todo[inline]{change title\ldots}

qualitative evaluation findings:
how many true findings in relation to false positives does it find + average number of findings per app (remember that the findings now are for all 626 apps)

pick a couple of high scoring findings and explain the failure modes (eg doesn't take branching into account, etc)
+ of course the REAL bug + some real smells

and of course regarding the ``results'' always qualify -> this only for android, open source apps, manual evaluation, blabla, so we never actually know for sure

\subsubsection{RQ2.1: Is there a noticeable difference between the different adaptions?}
each for the slightly adapted forms (no context, class merged, etc)

\subsubsection{RQ2.2: Given a known missing method call bug, can this approach detect it?}
only if time!

reverse test: find actual missing call in bug database -> can I find it using the approach and is it among the top findings?

\subsection{RQ3: How meaningful are the benchmarking results?}
does it make sense to evaluate this method using the benchmarking as proposed by Monperrus et al.?
-> comparison between manual evaluation of findings vs automatic benchmark over the different techniques used (context, no context, class merge, \ldots)

first present benchmark results of different techniques (context, class merge, \ldots) then relate to qualitative results

are the results better if we leave out the context / merge on a per class basis / FOR DIFFERENT Ks!\ldots

\subsection{RQ4: How robust is this technique in the face of erroneous input data?}
I would add this, even if the results of comparison with manual evaluation are relatively negative / don't say anything

Robustness explanation
    wie viel brauche ich um sinnvoll viel zu lernen - hälfte der leute machen fehler -> wie robust brauchst du die daten / ist das verfahren !
    what is needed for it to work (lines of code, feature richness, etc) 
    problem: how to evaluate when it is "working" vs not working -> again simulate "missing methods", compare performance?

additional test:
	for true findings, try to throw away parts of the data -> when can we not find this anymore -> mathematical answer!
    / instead of throwing away: introducing random errors in the related tus

\subsection{RQ5: What are the requirements for the input size?}
ie: how big of a codebase do we need to be able to ``learn''. When can we apply this to a project which does not rely on some well known open source framework (Android) where it is easy to gather additional data, but on for example an in-house-closed-source library oä.

extremely hard to answer especially if the findings regarding benchmark validity are more or less negative.
however, even if I cannot give experimental answers, I can at least give some lower bounds + thoughts
(need at least 10 tus within the same ``category'' to even reach a score > 0.9, from practical results in qualitative analysis, that is probably not enough, blabla)

\section{Threats to Validity}

explain problems with the automatic evaluation
how related are the degraded TUs to missing method calls you can find in the wild?
improving the metrics by dropping cases where we know we won't find an answer

also mention runtime!
wie sehr sind die resultate aus der Android case study 1. Wahr (subjektive bewertet etc) 2. Übertragbar auf andere Anwendungsfälle(sind open-source programme of vergleichbar mit professionellen, Android eco System mit anderen, etc)
\ldots
