\chapter{Evaluation}\label{ch:eval}

\todo[inline]{
goal of evaluation: groÃŸe frage: bringt das Verfahren etwas?
understand how well this method works in the context of the android ecosystem, and the NATURE of findings

does this need something more?
/ point out the threats to validity / problems / improvement / smth?
}

% Which questions are we trying to answer with the evaluation?
Our evaluation of the method proposed by Monperrus et al.\ is aimed at comprehending how well the majority rule is suited to detect missing method calls in object oriented software.
We would like to understand better the nature and quality of findings produced by our implementation.
To achieve this, we apply our implementation to a large dataset of Android applications and evaluate the results qualitatively and quantitatively.
% chapter outline
In the following, we first give an overview of our methodology before we examine the dataset.
We present some general data and analyze how type usages behave in Android applications.
We discuss the results of the evaluation in response to 5 research questions which are all aimed at shedding some light on the applicability of this method.
Finally, we consider the meaning of these results and what might threaten their validity.

\section{Methodology}

% The introduction to your methodology section should begin by restating the research problem and underlying assumptions underpinning your study
In this thesis we explore an approach for detecting missing method calls.
We present an implementation of the method proposed by Monperrus et al.\ as well as two other related variations.
The goal of the evaluation is to understand how they behave and what kind of results one can expect.
\todo{more?}
% so what we are doing
Our experimental evaluation is twofold: a qualitative evaluation of the findings in 10 randomly selected Android apps and a simulation experiment using artificially created errors.
For the qualitative evaluation, we manually check each anomalous type usage and flag it as a true or false positive.
For the simulation, we remove method calls from the code and checking if our implementation can detect this known missing method call.
Since it is not assured that the results of this simulation correspond to the real performance, we also attempt to evaluate the applicability of the simulation experiment.
\todo{maybe a bit nicer / more clear / less information?}

\subsection{Qualitative Evaluation / Android Case Study}
\todo[inline]{decide on title\ldots}

% general description + source of android apps
F-droid\footnote{\url{https://f-droid.org/en/}} is a repository for Free and Open Source software (FOSS) on the Android platform.
We used a scraper to download the latest version of all available applications on the 6th of March 2018.
From these 625 Android applications, we extract all type usages that are present and apply our implementation of the $\text{DMMC}$ system to identify any anomalies.

% idea behind using android
The Android ecosystem is particularly suited for this evaluation and enables us to evaluate how type usages and the majority rule behave on real software.
The Java code is easy to analyze and there is a rich open source community, placing many sample applications at our disposal.
Each of these applications uses the Android API, so that we can generate a large dataset on the classes in the API, and detect any common patterns that emerge when using it.
Because of the API's emphasis on the user interface, we consider this to be an environment in which patterns will appear most frequently and most clearly.
\todo{formulate last sentence a bit different}

% actual steps to evaluation
\todo[inline]{ past tense makes sense or not really?  }
The detailed procedure of the manual evaluation was as follows:
We extracted the type usages of the 625 Android applications that we had downloaded and stored them in the database.
Next, we analyzed the extracted type usages with the 3 different variants $\text{DMMC}$, $\text{DMMC}_{\text{noContext}}$ and $\text{DMMC}_{\text{class}}$ and determined their respective $\operatorname{S-score}$s.
We consider type usages with an $\operatorname{S-score}$ bigger than $0.9$ to be anomalies.
This is the value that Monperrus et al.\ suggest and since it implies that 90\% of similar type usages are almost similar it seems like a reasonable cutoff.

While we would have liked to review all anomalies that our implementation uncovers, there were simply too many.
\todo{determine how many anomalous findings we have in total (easy!)}
Instead, we used a random number generator\footnote{Google frontpage on: \url{https://www.google.com/search?q=random+number}} to sample 10 applications to be examined in detail.
We manually reviewed all anomalous type usages of the selected applications and carefully considered the source code and the API documentation before classifying them as one of the following:
\begin{description}
    \item [real bug (B)] a real defect, change definitely necessary
    \item [real smell (S)] another way of doing things would be better, but probably not a defect in the given instance
    \item [hint (H)] the code is fine, but the pattern is also legitimate; some connection between the methods
    \item [false positive (FP)] the usage is totally fine; no causal relation implying the ``missing'' method must be called
\end{description}

\todo[inline]{
    descriptions fine like this?, don't need the ones below?
[Special case(?) (SC)] "weird" implementation with extra comment, for compatibility reasons, something like that
[not clear (NC)] source code not available, ...
}

Depending on the quality of results one expects, hints could either be counted towards smells or towards false positives.
We classify type usages as hints when it makes sense that the pattern exists, because many people will use the methods together and there is some causal connection, but, strictly speaking, there is nothing wrong with in the code in question.
As an example, consider the situation in which a developer calls \code{next} on an iterator that he just obtained from a list of size one.
Here, he knows that the iterator contains one element so he does not need to check for it with \code{hasNext}, nonetheless, it is useful to detect locations where \code{next} is invoked without a preceding \code{hasNext}.

\todo[inline]{
a bit more explanation?
    proper example code for better understanding?
    just use the easiest example? -> height and width pairs

additional examples:
only height() or width()
setTitle not called -> not strictly necessary but ``makes sense''
}

\subsection{Automated Benchmark}

% general description what its doing + reasons for doing it
We would like to understand better how the majority rule behaves under different circumstances.
Unfortunately, we do not have a large dataset of known missing method calls available that we could use to asses the quality of our implementation.
Instead, we create instances of missing calls by sampling a type usage from the dataset and removing one of its method calls.
We then calculate the strangeness score of the degraded type usage and determine if the majority rule can detect the known missing call.
Using this procedure, we can test the system on many cases for that we know the correct answer.
Knowing the expected answer, enables us to calculate relevant metrics that capture the systems success and compare the different variants on an objective basis.

% actual setup + degradation mechanism
We perform the benchmark using a custom benchmarking infrastructure written in Python.
It randomly chooses type usages from the dataset and applies a degradation mechanisms to it.
The normal degradation mechanism removes each method call of the selected type usage once and checks for each new version if our implementation of the $\text{DMMC}$ system detects the known anomaly.
To retain a reasonable performance and execution time, we only apply this procedure to a portion of the entire dataset (around 10\%).
\todo{correct percentage?}
For further evaluation, we also support different processes for degradation, such as removing more than one method call or degrading more than one type usage.

% how do we evaluate stuff
Since we know the correct answer for all degraded type usages, we can use the results to calculate a number of metrics.
\paragraph{Correct} is the percentage of queries that were detected as anomalous.
\paragraph{Precision} is the 
\paragraph{Recall}

\todo[inline]{
    explain correctly\ldots
+ those numbers in relation to nr tus / method list length
}

\todo[inline]{
additional metrics from recommender systems book p.245f (robustness, learning rate - p. 261)
performance -> training time + performance when working
already mention here that this kind of simulation is kinda questionable?}

\section{Dataset Overview}

\todo[inline]{
mention somewhere: the monperrus2013 p11 highlight? (at least equivalent for my dataset)
Does the Strangeness Score behave as expected on Android Apps? (the behavior that I'm expecting is basically a mathematical necessity!)
    do the general assumptions hold? (most tus have a low score, most apps have few findings, etc)
    -> most tus are ``normal'', a few are ``abnormal''
    what do those assumptions mean? -> they expect some kind of uniformity to the type usages, probably mostly present in GUI etc frameworks
}

% general dataset
%    How many Apps + How many TUs in total + each (avg / median) -> how big are the apps, (how are the tus split between the apps)
Our dataset consists of 3,880,556 type usages that the Java program extracted from the 625 Android applications downloaded from F-droid.
On average there are X type usages per application and the median is Y.
In Figure X we have plotted the number of type usages per application.
\todo[inline]{Plot TUS / applications}
Most applications are rather small with a few outliers responsible for most of the type usages.
% type usages:
%    percentage on Android framework, percentage on other stuff
Of all the type usages in the dataset, $7.66\%$ operate on types from the Android framework (i.e., their fully qualified package name starts with \code{android.*}).
%    length of method list -> histogram?
In Figure Z we have plotted how many method calls there are per type usage to give a feeling for the amount of information they provide.
\todo[inline]{Plot method list lenghts}

%   number of partitions (types + context / types) + maybe verteilung of number of tus per partition (that seems like an interesting / important one!)
In the variation $\text{DMMC}$, a partition is a valid combination of type in context and there are 1,410,709 combinations like that in the dataset.
In the variant $\text{DMMC}_{\text{noContext}}$, the number of partitions is equal to the number of types and the whole dataset includes a total of 202,988 different types.
Since $\text{DMMC}_{\text{class}}$ relies on $A'_\text{noContext}$, the partitions for it are the same.
However, because we merge the type usages before analyzing them, the total number of type usages for this variant goes down to XYZ.
\todo[inline]{determine class merge no of tus (can just load results)}
In Figure X we have plotted the distribution of type usages per partition for the different variants.
Observer that\ldots 
\todo[inline]{Plot verteilung of TUs per partition!, depending on variant}

% performance: total time +
We perform all our experiments on a MacBook Pro with a Intel\textregistered Coreâ„¢ i7-3720QM CPU @ 2.60GHz and 16GB of RAM.
Extracting the type usages from all applications took a total of 3 hours and 27 minutes, which is an average of around 20 seconds per application.
\todo[inline]{
plot duration of tu extraction in relation to number tus (proxy for project size)
mention somewhere that analysis itself is relatively cheap (especially for new project coming in)
}

\todo[inline]{
strangeness score:
    verteilung of strangeness score (histogram)
    graphs related to general score verteilung
    explain why we expect the scores verteilung to be like that
    score in relation to smth? ie method length, nr tus in partition, etc
    put this only in rq1?  -> not so sure\ldots
    DEFINITELY somewhere mention the average number of anomalies per app!
}

\section{Results}

\todo[inline]{
general idea:
qualitative evaluation: are the findings useful in practice?
quantitative evaluation: given some assumptions, how useful can we expect this technique to be? robustness? Qualitative vs automatic same results?
make sure I will be able to a) answer those questions and b) the answers are ``interesting''
}

We investigate the performance of our implementation along the lines of five research questions.
The goal of this investigation is to understand how useful we can expect this technique to be in practice.
We would like to understand some of the preconditions it requires and under which circumstances it might fail.
For each question, we first explain its purpose and modus of evaluation, before giving results and an interpretation of them.

\subsection{RQ1: How many true versus false Positives are among the anomalous Type Usages flagged by the Majority Rule?}

% motivation behind question + general
It is among the primary goal of our evaluation to understand if the method we implemented is something that a developer would use to help with maintenance.
It seems that the deciding factor for this would be the ability of the majority rule to detect bugs and to detect them with clarity.
If the developer needs to sift through thousands of anomalies to detect a single mistake, the technique is not very useful.
If, on the other hand, most of the high ranked anomalies are indeed hints for potential bugs, it would be of tremendous worth.

To mimic the experience a developer would have when applying our tool to his software, we randomly select 10 applications for in detail analysis.
Of these apps we examine each of the anomalous type usages in detail and classify them as described above.
Our measure of success shall be the number of findings per application and the ratio of true to false positive findings.
\todo{mention only here that we do the random sampling instead of above?}

% concrete techniques and results
\todo[inline]{
    prepare table with findings per app (+ avg/median)
    comment it in some useful manner!
}

\begin{figure}[t]
    \centering
    \begin{tabular}[h]{c|l|r|c|c|c|c|c}
\toprule
Nr & Name & Type Usages & Findings & B & S & H & FP \\
\midrule
1 & ar.rulosoft.mimanganu\_78.apk 			& 11241  & 12 & 1 & 1 & 4 &   6 \\
2 & com.zeapo.pwdstore\_94.apk 				& 12659  & 1  &\cc&\cc& 1 & \cc \\
3 & net.bitplane.android.microphone\_7.apk 	& 80     & \cc&\cc&\cc&\cc& \cc \\
4 & com.quaap.dodatheexploda\_2.apk			& 87     & \cc&\cc&\cc&\cc& \cc \\
5 & com.health.openscale\_23.apk 		   	& 3645   & 3  &\cc& 1 & 2 & \cc \\
6 & se.tube42.kidsmem.android\_16.apk 		& 13547  & 1  &\cc&\cc& 1 & \cc \\
7 & org.billthefarmer.diary\_125.apk	   	& 880    & \cc&\cc&\cc&\cc& \cc \\
8 & org.ligi.blexplorer\_12.apk 	    	& 8350   & \cc&\cc&\cc&\cc& \cc \\
9 & org.kaqui\_27.apk				    	& 1675   & \cc&\cc&\cc&\cc& \cc \\
10 & com.afollestad.nocknock\_13.apk     	& 9547   & \cc&\cc&\cc&\cc& \cc \\
\bottomrule
    \end{tabular}
    \caption{The results of the manual evaluation}\label{fig:manual}
\end{figure}

\subsubsection{RQ1.1: Is there a noticeable difference between the different variants?}

In Chapter \ref{ch:ext}, we suggested the variant $\text{DMMC}_\text{class}$ and Monperrus et al.\ already propose the $\text{DMMC}_\text{noContext}$ version.
To understand how they behave in comparison to the $\text{DMMC}$ system, we applied them to the same 10 applications that we randomly selected for the manual evaluation.
Once more we review all of their findings and classify them as true or false positives.

\todo[inline]{table with the results + discussion!, replace names with numbers\ldots
    mention some numbers related to the failure modes? (still not sure if I wanna do their discussion here or below)
}

\begin{figure}[t]
    \centering
    \begin{tabular}[h]{r|r|c|c|c|c|c|c|c|c|c|c}
\toprule
%\cmidrule(l){3-10}
\multicolumn{2}{c}{ } & \multicolumn{5}{|c|}{noContext} & \multicolumn{5}{|c}{class} \\
     \midrule
App & Type Usages & Findings & B & S  & H    & FP & Findings & B & S & H & FP \\
\midrule
1 & 11241  & 41 (24) & 1 (1)  & 1 (1) & 20 (15) & 19 (7) & 28 &  1  &  \cc &  8  &  19 \\
2 & 12659  & 16 (3)  &  \cc   &  \cc  &  \cc    & 16 (3) & 10 & \cc &  \cc &  1  &   9 \\
3 & 80     &  0 (0)  &  \cc   &  \cc  &  \cc    &  \cc   &  0 & \cc &  \cc & \cc & \cc \\
4 & 87     &  0 (0)  &  \cc   &  \cc  &  \cc    &  \cc   &  0 & \cc &  \cc & \cc & \cc \\
5 & 3645   &  8 (6)  &  \cc   &  \cc  &  3 (1)  &  5 (5) &  7 & \cc &  \cc & \cc &   7 \\
6 & 13547  & 23 (0)  &  \cc   &  \cc  &  7 (0)  & 16 (0) &  0 & \cc &  \cc & \cc & \cc \\
7 & 880    & 21 (2)  &  \cc   &  \cc  &  \cc    & 21 (2) & 21 & \cc &  \cc & \cc &  21 \\
8 & 8350   &  4 (4)  &  \cc   &  \cc  &  \cc    &  4 (4) &  2 & \cc &  \cc &  1  &   1 \\
9 & 1675   &  2 (1)  &  \cc   &  \cc  &  2 (1)  &  \cc   &  1 & \cc &  \cc &   1 & \cc \\
10& 9547   & 15 (9)  &  \cc   &  \cc  &  \cc    & 15 (9) &  4 & \cc &  \cc & \cc &   4 \\
\bottomrule
    \end{tabular}
    \caption{Comparing $\text{DMMC}_\text{noContext}$ and $\text{DMMC}_\text{class}$. The numbers in brackets indicate findings on the project itself.}\label{fig:manual2}
\end{figure}
\todo{include the SC findings or not?
    2 -> 29 (2) SC
    8 -> 24 (0) SC

visualize that its worse? aka make the ones where it's higher red oÃ¤?
switch toprule to smth else to not go over whole table
}

%\subsubsection{RQ1.2: Given a known missing method call bug, can this approach detect it?}
% only if time! -> probably not
% reverse test: find actual missing call in bug database -> can I find it using the approach and is it among the top findings?

\subsection{RQ2: Do the Benchmark Results align with the Results of the Manual Evaluation?}
\todo[inline]{IMPROTANT: this as a sub question?! / at least have some question related to the benchmark and no tus and length of method list and precision}

\todo[inline]{edit for concision and coherence}
% general
Monperrus et al.\ base a significant part of their evaluation on an automated benchmark much like the one we perform.
While such a simulation makes sense on the face of it, one has to question how meaningful are its results.
Recall that we essentially simulate the situation that a developer forgets the method call we removed and verify if our implementation would be helpful in this case.
The critical assumption here is that the artificially created missing calls are in some way comparable to missing method call bugs in real software.
If they are not, the benchmark only measures how well the method performs on the exact problem of detecting randomly removed method calls, which is not very helpful at all.

% how we want to evaluate the simulation itself
For us it is not inherently clear that the degenerated type usages behave like real missing method calls.
Because of this, we would like to present the benchmark results twofold.
First, we present the benchmark results of the different variants $\text{DMMC}$, $\text{DMMC}_\text{noContext}$ and $\text{DMMC}_\text{class}$.
Second, we compare the results of the manual evaluation to the benchmark results and discern whether the performances align.
In the end, we would like to answer, if it make sense to evaluate this method using degenerated type usages as proposed by Monperrus et al.
\todo[inline]{
a bit better explanation\ldots
second put them in relation to the results of the manual eval -> do we get similar performance between the variants?
    compare results to the results of manual evaluation between the different methods (ie does one perform much worse in both or just in one?)
    -> comparison between manual evaluation of findings vs automatic benchmark over the different techniques used (context, no context, class merge, \ldots)
+ mention the technique:
    simply give metrics (precision, recall) for the different variants and relate them to the manual results
}

\todo[inline]{
present results!
(maybe relate the precision etc to the number of tus in the partition -> already have that data)
}

\subsection{RQ3: How robust is this technique in the face of erroneous input data?}

\todo[inline]{
worauf is robustness bezogen? -> results quality or smth else? (Robustness explanation)
what is needed for it to work (lines of code, feature richness, etc) 
wie viel brauche ich um sinnvoll viel zu lernen - hÃ¤lfte der leute machen fehler -> wie robust brauchst du die daten / ist das verfahren !
}

% general / motivation
Apart from the quality of the results, it is also important to understand what other requirements this technique has.
Here, we would like to examine how sensitive the majority rule is to perturbed inputs.
Since we extract type usages and then patterns from any input code, it is entirely feasible that there will be some erroneous code among it.
Imagine for example a new library that is not very well designed causing many developers to use it in incorrect ways.
Now the question is, how much correct input do we need to detect the incorrect usages?

% problem + math answer
problem: how to evaluate when it is "working" vs not working
    -> again simulate "missing methods" + additional erros, compare performance?
    but problematic benchmark ``doesn't work at all''
definitely also mention some MATH
    how does the strangeness score behave if there are additional erroneous instances
    if there is only one anomaly and we get another one (of the same type) can half the strangeness score
    -> the thing regarding if the same method is removed score halfs / if it's another one it's not so bad (should have written down somewhere right?)
    or just decrease it in proportion to the almost equal number (if wrong method gets removed)
    + probably ppl make a similar mistake assuming it's because this api is hard to use oÃ¤ -> for this it might be hard
    but assumption: if one method is difficult, it's most likely the one ppl forget
example: use true bug finding, try to throw away parts of the data -> when can we not find this anymore -> mathematical answer!
/ instead of throwing away: introducing random errors in the related tus

% techniques + results
\todo[inline]{
include here the benchmark results (I have them so i might as well)
try with intentionally degraded data -> I already have the setup, I just have to improve the benchmark thing performancewise a bit and let it run
}

\subsection{RQ4: What are the requirements for the input size?}
\todo[inline]{
benny didn't like ``requirements''
-> How much input is needed to produce 'useful' results?
How big does the system under analysis need to be?
How many type usages are necessary to \ldots
}

% motivation + general
besides the sensitivity to low quality input as discussed in the previous section, we are also interested in how much data this method needs to produce sensible results
ie: how big of a codebase do we need to be able to ``learn''. When can we apply this to a project which does not rely on some well known open source framework (Android) where it is easy to gather additional data, but on for example an in-house-closed-source library oÃ¤.
extremely hard to answer especially if the findings regarding benchmark validity are more or less negative.

% math answer
however, even if I cannot give experimental answers, I can at least give some lower bounds + thoughts
(need at least 10 tus within the same ``category'' to even reach a score > 0.9, from practical results in qualitative analysis, that is probably not enough, blabla)

% techniques + results
-> mention some of the parition sizes which I have already plotted during the dataset overview
potentially mention the number of ae / e for the different errortypes (B / S / FP)

\subsection{RQ5: Can the Majority Rule also detect superfluous or wrong Method Calls?}

% motivation + intro
in chapter X we extend the method for detecting missing method calls that monp et al proposed to also detect two other anomalies:
superfluous and wrong method calls
to understand if theses variations offer anything of value we evaluate their findings manually in the same manner as we evaluate the findings of the vanilla variant.

manual eval of results when using superfluous / wrong variants
-> same mode of evaluation as RQ1

already problem I'm seeing: things with 1 call will always be almost similar since only one call is different
 -> this could be a problem?

\section{Discussion}

% general
summary of what i did + results
also mention runtime!
general aussage like ``its good or its useless''

% general qualifiaction / threts to validity
and of course regarding the ``results'' always qualify -> this only for android, open source apps, manual evaluation, blabla, so we never actually know for sure
explain problems with the automatic evaluation
how related are the degraded TUs to missing method calls you can find in the wild?
improving the metrics by dropping cases where we know we won't find an answer

wie sehr sind die resultate aus der Android case study 1. Wahr (subjektive bewertet etc) 2. Ãœbertragbar auf andere AnwendungsfÃ¤lle(sind open-source programme of vergleichbar mit professionellen, Android eco System mit anderen, etc)

average number of findings per app (remember that the findings now are for all 626 apps)
how is the quality of findings (RQ1), how many findings are there, how much work is it to evaluate them

% Manual eval examples
qualitatively describe some of the failings?! 
pick a couple of high scoring findings and explain the failure modes (eg doesn't take branching into account, etc)
+ of course the REAL bug + some real smells
mention the remedies we have thought of (filters etc) but also -> say that it would be kinda backwards from the initial idea to stay away from hand crafted rules
-> not many kotlin apps -> easy to be an outlier for some stuff (init for example), etc
(muster: if there is "several" equal ones it doesn't work so well, if its just very few (<4) it's at least a smell?)

example: StringBuilder -> often fails for REASons, but can be easily filtered -> reduce nr of fps
-> these filters also as my ``contribution''? :D 
failure mode: doesn't follow static functions (where the method in question is actually called)
failure mode: kotlin -> outlier generally
suggested improvement: missing <init> basically never valid, usually static function, kotlin oÃ¤ -> remove?
dot chaining -> hard to fix / tradeoffs

mention the android.* filter! / does it actually make sense -> data says not really, rather filter on own app\ldots + that only in the end I'd say
consider filtering for usages that occur inside the app (by package) to exclude libraries (I think it makes sense...)
+ add the android filter (this will only reduce the number of findings, but maybe improve the quality?!)
+ add the ``only from my project'' filter

mention in the end that i tried a 
Better anomaly detection (is the anomality rule actually a GOOD measure for this kind of anomalies, or should we use something totally different?))
    clustering detector try (+ hypersphere idea?)
    but all in all it didnt work out / not enough time
    + vermutungen warum es nicht funktioniert -> dataset exploration + size of method and typeusage lists per partition!

mention this when discussing the results!
    static functions evaluation!
    something to fix the dotchaining problems
    -> for both basically I'm just saying that i looked into it and it was too much work / impossible?

