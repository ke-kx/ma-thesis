\chapter{Related Work}\label{ch:relWork}
%todo Position this here or just before conclusion? - right now here seems better though

\todo{generally in this section: think about the tense I wanna use, present / past and if it works well! + stick to it!}

%todo somewhere difference between smell and bug herausarbeiten
\todo[inline]{new ideas:
    mention code smells and their origin?, less focus on ``bugs'' per se (see aDoctor paper) -> while the thesis title is related to bugs, I think I can / want to use smell as well 
    Reference die belegt, dass es eine Korrelation zwischen bugs und smells gibt look in fontana2016comparing, has references to this in introduction
}

%--- relevance of code smells and finding them
The process of realizing that there exists a bug in a software system, identifying its cause and understanding the steps necessary to remove it, is difficult and time consuming.
Especially for larger software systems automatically detecting low quality code and improving it can contribute significantly to the maintainability of the code and prevent bugs in the future.
Thus, there has been a lot of interest in approaches for automatically detecting bugs or even just smells in code.

%--- section summary
In this chapter we will present a number of different approaches for smell and bug detection.
We start with some ``conventional'' methods which mostly rely on hard coded rules and pattern matching(?).
We then give a quick overview of smell detection specifically related to android applications, because we are applying the method studied in this thesis to a number of android apps in the evaluation section.
Finally, we look into techniques which attempt to learn rules and properties from the code they are analyzing instead of relying on rules which were given by the developer a-priori.
Such techniques have the advantage of needing fewer manual oversight and changes when the system which is to be tested evolves.

\section{Finding Code Smells with Static Analysis}
%todo section title like this or smth different? (hard coded rules, ..)

%todo improve flow\ldots and bug pattern what?
Findbugs\footnote{\url{http://findbugs.sourceforge.net/}, in the meantime there has been a successor: \url{https://spotbugs.github.io/}} is a static analysis tool that finds bugs and smells in Java code.
It reports around 400 bug patterns and can be extended using a plugin architecture.
Each bug pattern has its own specific detector, which uses some special, sometimes quite complex detection mechanism.
These detectors are often built starting from a real bugs, first attempting to find the bug in question and then all similar ones automatically as well.

In their work Ayewah et al. \cite{ayewah2007evaluating} apply it to several large open source applications (Sun's JDK and Glassfish J2EE server) and portions of Google's Java codebase.
\todo{links!}
Their premise is, that static analysis often finds true but trivial bugs, in the sense of these bugs not really causing a defect in the software.
This can be because they are deliberate errors, occur in situations which cannot happen anyways or situations from which recovery is not possible.
In their analysis Findbugs finds 379 medium and high priority warnings which they classify as follows:
\begin{itemize}
    \item 5 are due to erroneous analysis by Findbugs
    \item 160 are impossible or have little to no functional impact
    \item 176 can potentially have some impact
    \item 38 are true defects which have substantial impact, i.e. the real behavior is clearly not as intended
\end{itemize}
The takeaway is, that this kind of static analysis can find a lot of true bugs, however there will also be a lot of false positives among them and it can be difficult to distinguish them.

To alleviate the problem of a high false positive rate among the findings reported by Findbugs, Shen et al. \cite{shen2011efindbugs} propose a ranking method which attempts to rank true bugs before less important warnings.
It is based on a principle they call ``defect likelihood''.
With the findings of a large project (in this case the JDK) as a basis, they manually flag each finding as a true or false positive, before using this data to calculate the probability that a finding is a true finding.
This likelihodd can not only be calculated for one specific bug pattern but also across categories and types with variance as a tiebreaker.
The resulting ranking can further be refined with user feedback, when it is applied to a specific project.
In their evaluation on three open source applications (Tomcat, AspectJ and Axis) they compare their ranking against the default severity ranking of Findbugs, which is basically a hardcoded value for each bug type.
Using cutoffs at 10\%, 20\%, 30\%, \ldots of the total findings, they achieve precision and recall systematically better than the default ranking.
This especially holds true for cutoff values around 50\%.

%todo Which other papers?!

\section{Android specific Smell Detection}

In Chapter \ref{ch:eval} we are analyzing a large number of open source android applications.
Thus, as a small overview of android related code smell detection consider the following papers.

Hecht et al. \cite{hecht2015detecting} presented an approach they call PAPRIKA which operates on compiled Android Apps, but tries to infer smells on the source code level.
From the byte-code analysis they build a graph model of the application and store it in a graph database.
The nodes of the graph are entities such as the app itself, individual classes, methods and even attributes and variables.
They are then further annotated with specific metrics relevant to the current abstraction level, e.g. ``Number of Classes'' for the app, ``Depth of Inheritance'' for a class or ``Number of Parameters'' for a method.
Finally, they extract smells using hand-crafted rules written in the Cypher query language\footnote{\url{https://neo4j.com/developer/cypher-query-language/}}.
This enables them to recognize 4 Android specific smells and 4 general, OOP related smells.

To evaluate this approach they developed a witness application, which contains 62 known smells.
Using the right kind of metrics, they where able to obtain precision and recall of 1 on this witness application.
Further, they apply their tool to a number of free Android apps in the playstore and make some assertions about the occurrence of antipatterns in publicly available applications.
One finding to emphasize is that some antipatterns, especially those related to memory leaks appear in up to 39\% of applications, even ``big'' ones like Facebook or Skype.

After Reimann et al. \cite{reimann2014tool} presented a catalogue of 30 Android specific code smells, Palomba et al. \cite{palomba2017lightweight} developed a tool to detect 15 of them.
It works on the abstract syntax tree of the source code and uses specifically coded rules to detect each of the smells.
For the evaluation of their approach they examine 18 Android applications by comparing the results of their tool against a manually built oracle.
The oracle was created by having two Masters students study the code of the applications in depth and manually flag offending locations.
While they reach an average precision and recall of 98\%, there are a number of cases in which their tool fails or yields false positives.

One especially interesting case is the smell of missing compression.
When making external requests it is advisable to compress the data to save bandwidth.
To detect places in the code where a request is made, but compression is missing, their hard-coded rule tests if one of the two popular compression libraries is used.
However, recently there has been a new competitor compression library gaining in popularity.
Their rule does not include it and, thus, falsely flags the usage of this library as the ``missing compression'' smell.
This is a great example of a case where hard-coded rules fail, because the are not kept up to date (new library becomes popular, interface changes, \ldots), do not consider a special case or otherwise hanging criteria.

\section{Inferring Properties from the Code at Hand}

As the results of the previous paper showed, it can be difficult and costly to keep hard coded detection rules up to date and relevant.
Because of this, there has been a lot of interest in approaches which adapt automatically to changing requirements by learning rules from the source code and looking for violations of those rules.

[Why is this potentially better than other approaches (actually learning an api vs static rules comes to mind),
also mention some stuff about recommender systems in general (it is the official thesis topic after all\ldots)]

Mention \cite{engler2001bugs} as probably the first paper which proposed the general idea behind DMMC -> learning from the code at hand, instead of using static predefined rules

Mention MUBench?!
    -> useful classification of api misuses
    Dataset of bugs related to API misuses
    \ldots

General: read the summary paper \cite{robillard2013automated} again and check which approaches are there and which could / should be mentioned
+ check to read stuff\ldots
+ for more information: check this summary paper / the 2017 one? / both?

Remember the general machine learning approaches which were not super successful, but at least learned a LOT
especially like general code smells for example

\section{Previous work on detecting missing method calls / object usages}
% -> api missuse detection

There have been a number of works concerned with finding patterns in object usages and using those to find potential bugs.
Most relevant for this work are two papers by Monperrus et al.\cite{monperrus2010detecting}\cite{monperrus2013detecting} which introduced the notion of almost similarity and are the primary inspiration for this work.
Their method considers the invocation of methods on an object of a particular type in a particular context a type usage.
Here, the context is nothing more than the name of the method in which the type usage occurs together with its signature (the types of the method parameters).
After mining a list of all the type usages present in a code base, they relate the number of exactly equal type usages to the number of almost equal ones.
Exactly equal means that context, type and method list are exactly identical, while almost equal means the same, only that the method list can contain one additional method.
This technique is explained in detail in Chapter \ref{ch:dmmc}.

\todo[inline]{
results of monperrus et al.:
}

Before this, Wasylkowski et al. \cite{wasylkowski2007detecting} introduced a method to locate anomalies in the order of methodcalls.
First, they extract usage models from Java code by building a finite state automata for each method.
The automata can be imagined similarly to the control flow graph of the method with instructions as transitions in the graph.
From these they mine temporal properties, which describe if a method $A$ can appear before another method $B$.
One can imagine this process as determining if there exists a path through the automata on which $A$ appears before $B$, which in turn implies that a call to $A$ can happen before one to $B$.
Finally, they are using frequent itemset mining \cite{han2006data} to combine the temporal properties into patterns.
% todo add short explanation of frequent itemset mining? I think not really relevant

In this work an anomaly also occurs when many methods respect a pattern and only a few (a single one) break it.
In their experiments they find 790 violations when analyzing an open source program and find 790 violations.
Manual evaluation classifies those into 2 real defect, 5 smells and 84 ``hints'' (readability or maintainability could be improved).
This adds up to a false positive rate of 87.8\%, but with an additional ranking method they were able to obtain the 2 defects and 3 out of 5 smells within the top 10 results.

In a related work Nguyen et al \cite{nguyen2009graph} use a graph-based representation of object usages to detect temporal dependencies.
This method stands out because it enables detecting dependencies between multiple objects and not just one.
% todo what does the branching thing even mean?
The object usages are represented as a labeled directed graph where the nodes are field accesses, constructor or method calls and branching is represented by control structures.
The edges of the graph represent the temporal usage order of methods and the dependencies between them.
% todo is this merge sort thing true?!
Patterns are then mined using a frequent induced subgraph detection algorithm which builds larger patterns from small patterns from the ground up, similar to the way merge sort operates.
Here an anomaly is also classified as a ``rare'' violation of a pattern, i.e. it does not appear often in the dataset in relation to its size.
In an evaluation case study this work finds 64 defects in 9 open source software systems which the authors classifies to 5 true defects, 8 smells and 11 hints, which equals a false positive rate of 62.5\%.
Using a ranking method the top 10 results contain 3 defects, 2 smells and 1 hint.

\todo[inline]{mention more related work from detecting mmc paper?}

\section{Other Ideas}
\todo[inline]{change title}

\cite{zhou2017analyzing}
already mentioned problem of incorrect API documentation
this often happens when a developer adapts or improves the code but forgets to change the documentation as well
they propose an automated approach to detecting ``defects'' in the api documentation
defect = two cases: either api is incomplete in describing some constraints
or description exists, but does not match the situation in the code
assumption: code is correct (has been extensively tested)

too detailed?!
technique: static analysis of the code, then pattern based natural language processing -> first oder logic formula
same for the api document: part of speech tagger (find specific parts which usually indicate some specific situation) + some heuristics -> restrictions and constraints -> FOL
the resulting first-order logic formula is fed into an SMT (satifiablity modulo theories) solver \cite{barrett2009satisfiability} to detect inconsistencies (if fol of code <> fol of doc)
is able to detect four classes of documentation defects

results:
a prototype implementation applied to java.awt and java.swing finds 1419 defects of which 81.6\% are true positives (check by manual evaluation of some masters students)
+ applied to some other packages but with the heuristics extractd from the previous two, find 1188 defect of which 659 are TP (precision 55.5\%).
-> lots of defects in a supposedly well documented JDK

\cite{fontana2016comparing}
different approach to code smells: machine learning
-> use manually evaluated code + tagged at appropiated places to train machine learning system
this work supports 4 basic smells (data class, large class, feature envy, long method)
compares 16 different approaches, all have a good performance, J48 and random forest are best, support vector machines are worst
[long enough? i think yes?]

questionable: how useful is this? these smells are also detectable using ``normal'' techniques?
