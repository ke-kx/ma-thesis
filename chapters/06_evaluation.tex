\chapter{Evaluation}\label{ch:eval}

% notes:
take some stuff from results intro
roter faden: results of manual analysis vs benchmark (ie. is context better than no context)
take their method + analyze it on a qualitative + quantitative basis on a big android dataset
goal of evaluation:
große frage: bringt das Verfahren etwas?
understand how well this method works in the context of the android ecosystem

% Which questions are we trying to answer with the evaluation?
In the following we present our evaluation of the method proposed by Monperrus et al.
We strive to understand how well the majority rule is suited to detect missing method calls in object oriented software.
For this we apply it to a large dataset of Android applications and evaluate the results qualitatively and quantitatively. 
\todo{paragraphs don't work well together + might need some other content?}

% chapter outline
We give an overview of the methodology used during the evaluation, explore the dataset we are using and how the type usage notion operates on it.
For the main evaluation we perform qualitative and quantitative evaluation on a randomly selected subset of the whole dataset.
We present the results in answer to X research questions which are aimed ad shedding some light on the applicability of this method.
Finally, we discuss the meaning of these results and how to understand them.

\section{Methodology}

% The introduction to your methodology section should begin by restating the research problem and underlying assumptions underpinning your study
The goal of the method we explore in this thesis is to detect missing method calls.
We use a method proposed by Monperrus et al. and some slight variations of it and would like to establish if and how well they work.
\todo{more?}
% so what we are doing
Our experimental evaluation is twofold: a qualitative evaluation of the findings in 10 randomly selected Android apps and a simulation experiment.
For the qualitative evaluation we manually check each anomalous type usage and flag it as a true or false positive.
The simulation consists of artificially removing method calls from the analyzed code and checking if we can this known missing method call.
Since it is not assured that the results of this simulation correspond to the real performance, we also attempt to evaluate the applicability of the simulation experiment.
\todo{maybe a bit nicer / more clear / less information?}

\subsection{Qualitative Evaluation / Android Case Study}

% general description + source of android apps
F-droid\footnote{\url{https://f-droid.org/en/}} is a repository for Free and Open Source software (FOSS) on the Android platform.
We used a simple scraper to download the latest version of all available applications on the 6th of March 2018.
From these XXX Android applications, we extract all type usages that are present and apply our implementation of the DMMC system to identify any anomalies.

% idea behind using android
We are using software from the Android ecosystem for the evaluation to understand how this method behaves on real software.
Advantages of Android are that it is based on Java and has a rich open source community.
We can generate a large dataset because of the wide availability of sample applications and each of them will facilitate the Android API.
Using this we can detect any common patterns that emerge when using the rich Android API.
Especially because of the emphasis on GUI we consider this one of the environments in which patterns will appear most frequent and most clearly.
\todo[inline]{needs rewriting, shuffling + making the points more clear\ldots
    maybe Start with the idea and then say android is good for this
do real software systems actually behave in this way ie: is it ``necessary'' to use classes in the same way
}

\todo[inline]{
    needs way better coherence!
    mention the android.* filter!
    change all dmmc to using text?
    reshuffle a bit
}
% actual steps to evaluation
The procedure of the evaluation is as follows:
We extract the type usages of all XXX Android applications that we downloaded and persist them into the database.
We then use the 3 different variants $\mathnormal{DMMC}$, $\mathnormal{DMMC}_{\mathnormal{noContext}}$ and $\mathnormal{DMMC}_{\mathnormal{class}}$ to analyze the type usages and determine any anomalous findings.
Monperrus et al. consider a strangeness score bigger than 0.9 to mark an anomaly and this is our cutoff as well.
We manually evaluate the anomalous findings and flag each of them as one of the following:

\begin{description}
    \item [real bug (B)] a real defect, change definitely necessary
    \item [real smell (S)] another way of doing things would be better, but probably not a defect
    \item [implementation error (on my side) (IE)] dot chaining not recognized correctly (not sure if actualy possible) (-> not sure i should include this)
    \item [false positive (FP)] the usage is totally fine, there is no causal relation which would imply that the "missing" method also needs to be used
	(subcategory: it makes sense that the pattern exists ie that many ppl use this together but it simply doesn't indicate an error or smell)
    \item [Special case(?) (SC)] "weird" implementation with extra comment, for compatibility reasons, something like that
    \item [not clear (NC)] source code not available, ...
\end{description}
\todo[inline]{make sure that these categories make sense and adapt descriptions to be more serious}

% how do we decide what is what?
the decision is based on reviewing the source code and api documentation


\subsection{Automated Benchmark}

maybe get some inspiration from monperrus description\ldots

% general description what its doing + resons for doing ti
general idea behind evaluation method and what we are trying to detect
actually it's a sort of simulation (see recommender system book p. 301f) - micro vs macro evaluation, \ldots
    imitation of the real system of software development
    using a much simpler system, namely dropping method calls
monperrus did it + seems reasonable
    obvious question: how similar are the such created mmcs to mmcs in the wild?

% actual setup + degradation mechanism
building benchmarking infrastructure
flexibility with python class setup
degrading type usages + checking if they will be detected
    some more subsections? (different ways for degrading type usages?)
    sometimes in even further degraded settings (degrade more than one)

% how do we evaluate stuff
Metrics explanation - Precision, Recall
    additional metrics from recommender systems book p.245f (robustness, learning rate - p. 261)
    performance -> training time + performance when working

% how we want to evaluate the simulation itself
compare results to the results of manual evaluation between the different methods (ie does one perform much worse in both or just in one?)

\section{Dataset Overview}

% general idea
general data about avg tus related to different settings etc -> what does our dataset even consist of?
mention somewhere: the monperrus2013 p11 highlight? (at least equivalent for my dataset)
Does the Strangeness Score behave as expected on Android Apps? (the behavior that I'm expecting is basically a mathematical necessity!)
    do the general assumptions hold? (most tus have a low score, most apps have few findings, etc)
        -> most tus are ``normal'', a few are ``abnormal''
        what do those assumptions mean? -> they expect some kind of uniformity to the type usages, probably mostly present in GUI etc frameworks

% motivation
should answer:
    1 What kind of dataset do we have?
    2 How are type usages in Android apps like?
    3 How does the strangeness score behave?
    4 Is it even fast enough? (this here or as separate research question?)
-> get general feeling for this method and the dataset
look into and understand the type of data i have -> how often small inputs, how often big, ...

% concrete questions to answer + way of presenting it
1 Dataset:
    How many Apps + How many TUs in total + each (avg / median) -> how big are the apps, (how are the tus split between the apps)
    number of partitions (types + context / types) + maybe verteilung of number of tus per partition (that seems like an interesting / important one!)
    different number of tus for class merge? -> yes necessarily
        mention how the dataset becomes smaller -> need to see if results become better

2 type usages:
    length of method list -> histogram?
    percentage on Android framework, percentage on other stuff

3 strangeness score:
    verteilung of strangeness score (histogram)
    graphs related to general score verteilung

4 performance:
    total time +
    duration of tu extraction in relation to number tus (proxy for project size)
    + mention somewhere that analysis itself is relatively cheap (especially for new project coming in)

\section{Results}

qualitative evaluation: are the findings useful in practice?
quantitative evaluation: given some assumptions, how useful can we expect this technique to be? robustness? Qualitative vs automatic same results?
make sure I will be able to a) answer those questions and b) the answers are ``interesting''

for each question: explanation of question, then results + interpretation

\subsection{RQ1: How many true versus false Positives are among the anomalous Type Usages flagged by the Majority Rule?}

% General
wie relevant sind diese obtained results in der praxis (as determined by fp vs tp)
how many true findings in relation to false positives does it find + 

% motivation behind question
is this something that a developer would use to help with maintenance?
are the results useful / helpful

% concrete techniques and results
start with avg + median number of findings per app + in relation to their total TUs
only the results of manual evaluation of the 10 random apps with the context variant
+ add the android filter

\subsubsection{RQ1.1: Is there a noticeable difference between the different variants?}

do the manual eval each for the slightly adapted forms (no context, class merged) and see if there is any difference
+ include the android filter

\subsubsection{RQ1.2: Given a known missing method call bug, can this approach detect it?}
only if time! -> probably not

reverse test: find actual missing call in bug database -> can I find it using the approach and is it among the top findings?

\subsection{RQ2: Do the Benchmark Results align with the Results of the Manual Evaluation?}

% general
how 'meaningful' are the benchmarking results?
does it make sense to evaluate this method using the benchmarking as proposed by Monperrus et al.?
-> comparison between manual evaluation of findings vs automatic benchmark over the different techniques used (context, no context, class merge, \ldots)
first present benchmark results of different techniques (context, class merge, \ldots) then relate to qualitative results
are the results better if we leave out the context / merge on a per class basis / FOR DIFFERENT Ks!\ldots

% motivation

% techniques + results
potentially simulate the benchmark only with tus from exactly the same 10 random apps? (so i don't have to do the whole dataset?)
simply give metrics (precision, recall) for the different variants and relate them to the manual results
(maybe relate the precision etc to the number of tus in the partition -> already have that data)

\subsection{RQ3: How robust is this technique in the face of erroneous input data?}

% general
I would add this, even if the results of comparison with manual evaluation are relatively negative / don't say anything
worauf is robustness bezogen? -> results quality or smth else?
Robustness explanation
    wie viel brauche ich um sinnvoll viel zu lernen - hälfte der leute machen fehler -> wie robust brauchst du die daten / ist das verfahren !
    what is needed for it to work (lines of code, feature richness, etc) 
    problem: how to evaluate when it is "working" vs not working -> again simulate "missing methods", compare performance?
additional test:
	for true findings, try to throw away parts of the data -> when can we not find this anymore -> mathematical answer!
    / instead of throwing away: introducing random errors in the related tus

% motivation

% techniques + results
I mean I have the benchmark test, so i might as well do it, but also mention mathematical results!

\subsection{RQ4: What are the requirements for the input size?}
\todo[inline]{
benny didn't like ``requirements''
-> How much input is needed to produce 'useful' results?
How big does the system under analysis need to be?
How many type usages are necessary to \ldots
}

% general
ie: how big of a codebase do we need to be able to ``learn''. When can we apply this to a project which does not rely on some well known open source framework (Android) where it is easy to gather additional data, but on for example an in-house-closed-source library oä.
extremely hard to answer especially if the findings regarding benchmark validity are more or less negative.
however, even if I cannot give experimental answers, I can at least give some lower bounds + thoughts
(need at least 10 tus within the same ``category'' to even reach a score > 0.9, from practical results in qualitative analysis, that is probably not enough, blabla)

% motivation

% techniques + results

\subsection{RQ5: Can the Majority Rule also detect superfluous or wrong Method Calls?}

manual eval of results when using superfluous / wrong variants
-> same mode of evaluation as RQ1

\section{Discussion}

also mention runtime!

average number of findings per app (remember that the findings now are for all 626 apps)
how is the quality of findings (RQ1), how many findings are there, how much work is it to evaluate them

and of course regarding the ``results'' always qualify -> this only for android, open source apps, manual evaluation, blabla, so we never actually know for sure

pick a couple of high scoring findings and explain the failure modes (eg doesn't take branching into account, etc)
+ of course the REAL bug + some real smells

\todo[inline]{
mention in the end that i tried a 
Better anomaly detection (is the anomality rule actually a GOOD measure for this kind of anomalies, or should we use something totally different?))
    clustering detector try (+ hypersphere idea?)
    but all in all it didnt work out / not enough time
    + vermutungen warum es nicht funktioniert -> dataset exploration + size of method and typeusage lists per partition!

mention this when discussing the results!
    static functions evaluation!
    something to fix the dotchaining problems
    -> for both basically I'm just saying that i looked into it and it was too much work / impossible?
}

\section{Threats to Validity}
\todo[inline]{needs to be extra section or can be together with discussion?}

explain problems with the automatic evaluation
how related are the degraded TUs to missing method calls you can find in the wild?
improving the metrics by dropping cases where we know we won't find an answer

wie sehr sind die resultate aus der Android case study 1. Wahr (subjektive bewertet etc) 2. Übertragbar auf andere Anwendungsfälle(sind open-source programme of vergleichbar mit professionellen, Android eco System mit anderen, etc)
