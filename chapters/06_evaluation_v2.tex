\chapter{Evaluation}\label{ch:eval}
% outline: https://thesisguide.org/2015/03/20/how-to-write-a-case-study/
\todo[inline]{see which parts should be sections or subsections and for which paragraph oä is enough}

% Which questions are we trying to answer with the evaluation?
Our evaluation of the method proposed by Monperrus et al.\ is aimed at understanding how well the majority rule is suited to detect missing method calls in object oriented software.
We apply our implementation to a large dataset of Android applications and evaluate the results qualitatively and quantitatively.
% chapter outline
In the following, we first present the research questions and why they are relevant for our understanding of this method.
We examine the dataset, present some general data on it and analyze the properties of type usages in Android applications.
After giving some details on the study design, we present the results of our evaluation in response to our research questions.
Finally, we consider the meaning of these results and what might threaten their validity.

\section{Research Questions}
% should contain: questions that the study aims to answer and their rationale, why they are relevant

Over the course of this evaluation, we would like to understand better the nature and quality of findings produced by our implementation of the $\text{DMMC}$ system for detecting missing method calls.
We answer the following 5 (6)? more detailed research questions.
\todo[inline]{smth more ala ``and what other prerequisites this technique has'' oä?}

\subsection{RQ 1: How many true versus false positives are among the anomalous type usages flagged by the majority rule?}

% motivation behind question + general
It is among the primary goals of our evaluation to understand if the technique we implemented is something that a developer would use to improve the quality of the software he is developing.
It seems that the deciding factor for this would be the ability of the majority rule to detect bugs and to detect them with clarity.
If the developer needs to sift through thousands of anomalies to detect a single true mistake, the technique is not very useful.
If, on the other hand, most of the high ranked anomalies are indeed hints for potential bugs, it would be of tremendous worth.

\subsubsection{RQ 1.1: Is there a noticeable difference between the different variants?}

In Chapter \ref{ch:ext}, we suggest $\text{DMMC}_\text{class}$ and Monperrus et al.\ already propose $\text{DMMC}_\text{noContext}$.
We would like to understand how these variations behave in comparison to the original $\text{DMMC}$ system, are the results they produce of comparable, better or worse quality?

\subsection{RQ 2: Do the benchmark results align with the results of the manual evaluation?}

% general description what benchmark doing + reasons for doing it
Monperrus et al.\ base a significant part of their evaluation on an automated benchmark with the aim of understanding better how the majority rules behaves under different circumstances.
Because they do not have a large dataset of known missing method calls available that they could use to asses the quality of their method, they create instances of missing calls by sampling a type usage from the dataset and removing one of its calls.
They then calculate the strangeness score of this degraded type usage and determine if their implementation can detect the known missing call.
Using this procedure, they can test the system on many cases for that they know the correct answer.
Knowing the expected answer, they can calculate relevant metrics that capture the systems success and compare the different variants on an objective basis.

% it seems to make sense, but is also a little bit questionable, we would like to confirm / deny if we can put some trust into the results
While such a benchmark makes sense on the face of it, one can question how meaningful are its results.
Monperrus et al.\ are essentially simulating the situation that a developer forgets the method call they remove and verify if their implementation would be helpful in that case.
The critical assumption here is that the artificially created missing calls are in some way comparable to real missing method call bugs in software.
If they are not, the benchmark only measures how well the implementation performs on the exact problem of detecting randomly removed method calls, which is not very meaningful at all.
\todo[inline]{
    anything else? is the question clear like this?
``In the end, we would like to answer, if it make sense to evaluate this method using degenerated type usages as proposed by Monperrus et al.''
}

\subsection{RQ 3: How many type usages are necessary to detect anomalies?}
\todo[inline]{ still not happy with title\ldots - removed ``accurately'' maybe better now? }

% motivation + general
Apart from investigating the quality of results, it is also important to understand what prerequisites this technique has.
Here, we examine how much data the majority rule needs to produce sensible results.
Ideally, we would like to understand how big a codebase needs to be before patterns and their violations emerge.
Can we apply this method to a in-house, closed-source library or does it only work on a well known open source framework like Android where it is easy to gather additional data?

\subsection{RQ 4: How robust is the majority rule in the face of erroneous input data?}

% general / motivation
It is not only important to understand how much data the majority rule requires, we are also interested how sensitive it is to perturbed inputs.
The type usages we extract originate from any input code and it is entirely feasible that there will be some erroneous code among it.
In fact, we expect some errors to be present, otherwise we could not hope to detect them.
However, the question is, how much correct input is needed to detect the incorrect usages?
Imagine, for example, a new library that is not very well designed causing many developers to use it in incorrect ways.
Can we expect the majority rule to be of any help in such a scenario or will it just fail completely?

\subsection{RQ 5: Can the majority rule also detect superfluous or wrong method calls?}

% motivation + intro
In Chapter \ref{ch:ext}, we propose extending the method for detecting missing method calls that Monperrus et al.\ introduced to also detect two other anomalies: superfluous and wrong method calls.
We would like to understand if the variations $\text{DMMC}_\text{superfluous}$ and $\text{DMMC}_\text{wrong}$ can successfully detect bugs in software.

\section{Study Objects} 
% outline software systems that are analyzed
% names + characteristics, why and how they where chosen + what are the consequences of the choice?

%--- general description of whole data + dataset overview!
% general description + source of android apps
F-droid\footnote{\url{https://f-droid.org/en/}} is a repository for Free and Open Source software (FOSS) on the Android platform.
We used a scraper to download the latest version of all available applications on the 6th of March 2018.
From the 625 Android applications obtained in this manner, we extract all type usages that are present and apply our implementation of the $\text{DMMC}$ system to identify any anomalies.

% idea behind using android
The Android ecosystem is particularly suited for this evaluation and enables us to evaluate how type usages and the majority rule behave on real software.
The Java code is easy to analyze and there is a rich open source community, placing many sample applications at our disposal.
Each of these applications uses the Android API, so that we can generate a large dataset on the classes in the API, and detect any common patterns that emerge when using it.
Because of the API's emphasis on the user interface, we consider this to be an environment in which patterns will appear most frequently and most clearly.
\todo[inline]{
lars: can we support this claim with smth?!
formulate last sentence a bit different + first one as well?
maybe a bit better rausarbeiten WHY we choose android and what are the CONSEQUENCES
}

% general dataset
%    How many Apps + How many TUs in total + each (avg / median) -> how big are the apps, (how are the tus split between the apps)
Our dataset consists of a total of 3,880,556 type usages that the Java part of our implementation extracted from the 625 Android applications downloaded from F-droid.
On average there are X type usages per application and the median is Y.
In Figure X we have plotted the number of type usages per application.
\todo[inline]{Plot TUS / applications}
Most applications are rather small with a few outliers responsible for most of the type usages.
% type usages:
%    percentage on Android framework, percentage on other stuff
Of all the type usages in the dataset, $7.66\%$ operate on types from the Android framework (i.e., their fully qualified package name starts with \code{android.*}).
%    length of method list -> histogram?
In Figure Z we have plotted how many method calls there are per type usage to give a feeling for the amount of information they provide.
\todo[inline]{Plot method list lenghts}

%   number of partitions (types + context / types) + maybe verteilung of number of tus per partition (that seems like an interesting / important one!)
In the variation $\text{DMMC}$, a partition is a valid combination of type and context and there are 1,410,709 combinations like that in the dataset.
In the variant $\text{DMMC}_{\text{noContext}}$, the number of partitions is equal to the number of types and the whole dataset includes a total of 202,988 different types.
Since $\text{DMMC}_{\text{class}}$ relies on $A'_\text{noContext}$, the partitions for it are the same as for $\text{DMMC}_{\text{noContext}}$.
However, because we merge the type usages before analyzing them, the total number of type usages for this variant goes down to XYZ.
\todo[inline]{determine class merge no of tus (can just load results)}
In Figure X we have plotted the distribution of type usages per partition for the different variants.
Observer that\ldots 
\todo[inline]{Plot verteilung of TUs per partition!, depending on variant}

\todo[inline]{
mention here: the monperrus2013 p11 highlight? (at least equivalent for my dataset)
    % Second, we note that the distributions of type-usages per type and type-usages per context follow a power-law distribution, which means that a few types and a few contexts trust a large number of type-usages. For instance, for eclipse-swt, the top-20 most popular types (out of 389) cover 62% of the type-usages. This goes along the same line as the results of Baxter et al. [Baxter et al. 2006]
}

% performance: total time +
We perform all our experiments on a MacBook Pro with an Intel\textregistered Core™ i7-3720QM CPU @ 2.60GHz and 16GB of RAM.
Extracting the type usages from all applications took a total of 3 hours and 27 minutes, which is an average of around 20 seconds per application.
\todo[inline]{
plot duration of tu extraction in relation to number tus (proxy for project size)
mention somewhere that analysis itself is relatively cheap (especially for new project coming in)
}

\todo[inline]{
strangeness score:
    verteilung of strangeness score (histogram)
    graphs related to general score verteilung
    explain why we expect the scores verteilung to be like that
    score in relation to smth? ie method length, nr tus in partition, etc
    put this only in rq1?  -> not so sure\ldots
    DEFINITELY somewhere mention the average number of anomalies per app!
Does the Strangeness Score behave as expected on Android Apps? (the behavior that I'm expecting is basically a mathematical necessity!)
    do the general assumptions hold? (most tus have a low score, most apps have few findings, etc)
    -> most tus are ``normal'', a few are ``abnormal''
    what do those assumptions mean? -> they expect some kind of uniformity to the type usages, probably mostly present in GUI etc frameworks
}

\section{Study Design}
% This section describes how the study, using the information from the study objects, attempts to answer the research questions.

\todo[inline]{check if i can formalize a bit better at places, aka what are the deciding factors for answering the questions}

\paragraph{For RQ 1: True vs False Positives}

To determine if an anomaly that was flagged by our implementation is a true finding, we need to manually review it.
Unfortunately, our implementation uncovered too many anomalous type usages to review all of them, prompting us to randomly select 10 applications for detailed analysis.
We examine each anomalous type usages of these applications in detail and classify them as either true or false positive.
This mimics the experience a developer would have when applying our tool to his software.
To answer the research question, we consider the total number of findings per application and the ratio of true to false positive findings.

For RQ 1.2, whether the variations $\text{DMMC}_\text{noContext}$ and $\text{DMMC}_\text{class}$ produce different results, we apply them to the same 10 applications that we sampled for RQ 1.
Once more, we review all findings and classify them as true or false positives.

\paragraph{For RQ 2: Benchmark Results}

% general procedure
To decide if the benchmark results coincide with the results of the manual analysis, we first perform the benchmark itself.
That is, we sample type usages from the dataset, degrade them by removing one of their calls and check if our implementation can detect these known anomalies.
Because we know the correct answer for all degraded type usages, we can use the results to calculate a number of metrics:
\begin{description}
    \item [Correct] is the percentage of queries that were detected as anomalous.
    \item [Precision] is the \ldots
    \item [Recall] is the \ldots
\end{description}
\todo[inline]{actually explain metrics!}
% how we want to evaluate the simulation itself
These metrics are at least mildly interesting by themselves if we compare them across the three variants $\text{DMMC}$, $\text{DMMC}_\text{noContext}$ and $\text{DMMC}_\text{class}$.
However, as we are not sure about their significance, we also want to evaluate the benchmark itself.
To do so, we compare the benchmark results of each variant to the results of the manual evaluation we performed for RQ 1.
We would expect them to align, that is if one variant produces bad results in the manual evaluation it should also yield bad results in the benchmark.
If there is a serious mismatch between the manual and the benchmark results, this would question the validity of the benchmark.
However, if they align, it would at least strengthen the idea that the benchmark has some relation to the real world performance of these techniques.

\todo[inline]{
rephrase end of this paragraph - the second part does not actually follow!
am i actually describing ``how to answer the question'' or smth else?
concrete measures for success or not sucess?
}

\paragraph{For RQ 3: Input Size}

Even if we have just questioned the validity of the benchmark, we still think that it can offer some insight into the working of our implementation.
Thus, to understand how the quality of findings depends on the amount of input data that is available, we perform a similar kind of benchmark as for RQ 2.
That is, we sample type usages, degrade them by removing a method call and then check if our implementation recognizes them as anomalies.
We calculate the same metrics as in the previous experiment, but this time we consider one additional factor: the size of the partition.
We aim to understand if the partition size has any influence on the precision of our implementation and if so, what partition sizes are advantageous.
In the discussion, we also present some mathematical considerations to support the results.
\todo[inline]{need to explain the partition size at least a little bit, at least reference previous section?
    + mention mathematical results here or not?}
    
\paragraph{For RQ 4: Robustness}

%threefold: mathematical considerations in discussion, benchmark, true BUG + take away
It is difficult to give an accurate answer to the question how sensitive the majority rule is to low-quality input data.
However, we would like to present three different points that should be taken into account.
The first one is a mathematical consideration with regards to the behavior of the $\operatorname{S-score}$, we give it in full in the discussion.
The second one is a simulation experiment similarly to the one perform for the previous question and for the third one we examine a true bug that our implementation discovered.
\todo[inline]{mention bug and math answers here or only in discussion? -> right now think only discussion would be best..}

% benchmark answer: idea
In the simulation experiment, we again sample a type usage from the dataset and degrade it to create a known missing method call.
However, this time, degrading only one type usage is not enough.
Instead, we pick a second random type usage from the same partition as the first one and also remove one of its methods calls, before checking if our implementation can detect the known missing call.
Thus, we simulate the situation that the type in question is in some way difficult to use and it has been used wrongly in multiple places.
We can then calculate the same metrics as before and investigate how this lower quality input data affects the performance of our implementation.
\todo[inline]{is this the true degradation mechanism?
+ maybe a bit more formal and ausführliche erklärung?}

\paragraph{For RQ 5: Other Anomalies}

To understand if the majority rule can also be used to detect superfluous or wrong method calls, we apply our implementation of $\text{DMMC}_\text{superfluous}$ and $\text{DMMC}_\text{wrong}$ to the 10 random applications we sampled for RQ 1.
We manually review their findings in the same manner as we review those of the original variant and label them as true or false positives.

\section{Study Procedure}
% This section describes the nitty gritty details required to implement the study design in reality.
% In principle, they could also be included directly in the description of the study design.
% However, it is easier for the reader to first understand the general idea, and then the details.

The only parameter that our implementation requires is the limit that defines when a type usage is an anomaly.
For this evaluation, we consider type usages with an $\operatorname{S-score}$ bigger than $0.9$ to be anomalies.
This is the value that Monperrus et al.\ suggest and since it implies that 90\% of similar type usages are almost similar it seems like a reasonable cutoff.

\subsection{Manual Review}

% concrete rnd nr gen + results? (already mention the apps -> reference the first rsult figure!)
We used a random number generator\footnote{Google frontpage on: \url{https://www.google.com/search?q=random+number}} to sample the 10 applications that should be examined in detail.
The names of the selected applications can be found in Figure \ref{fig:manual}.
% manual eval steps?
We analyze all of their type usages with the three different variants $\text{DMMC}$, $\text{DMMC}_{\text{noContext}}$ and $\text{DMMC}_{\text{class}}$ and determine their respective $\operatorname{S-score}$s.
We then manually review all anomalous type usages and carefully consider the source code and the Android API documentation before classifying them as one of the following:
\begin{description}
    \item [real bug (B)] a real defect, change definitely necessary
    \item [real smell (S)] another way of doing things would be better, but probably not a defect in the given instance
    \item [hint (H)] the code is fine, but the pattern is also legitimate; some connection between the methods
    \item [false positive (FP)] the usage is totally fine; no causal relation implying the ``missing'' method must be called
\end{description}

\todo[inline]{
    descriptions fine like this?, don't need the ones below?
[Special case(?) (SC)] "weird" implementation with extra comment, for compatibility reasons, something like that
[not clear (NC)] source code not available, ...
}

We consider bugs and smells to be true positives.
Depending on the quality of results one expects, hints could either be counted towards smells or towards false positives.
\todo{``results one expects'' not clear}
We classify type usages as hints when it makes sense that the pattern exists, because many people will use the methods together and there is some causal connection, but, strictly speaking, there is nothing wrong with the code in question.
As an example, consider the situation in which a developer calls \code{next} on an iterator that he just obtained from a list of size one.
Here, he knows that the iterator contains one element so he does not need to check for it with \code{hasNext}, nonetheless, it is useful to detect locations where \code{next} is invoked without a preceding \code{hasNext}.

\todo[inline]{
a bit more explanation?
    proper example code for better understanding?
    just use the easiest example? -> height and width pairs

additional examples:
only height() or width()
setTitle not called -> not strictly necessary but ``makes sense''
}

\subsection{Automated Benchmark}

% actual setup + degradation mechanism
We perform the benchmark using a custom benchmarking infrastructure written in Python.
It randomly chooses type usages from the dataset and applies a degradation mechanisms to it.
The normal degradation mechanism removes each method call of the selected type usage once and checks for each new version if our implementation of the $\text{DMMC}$ system detects the known anomaly.
To retain a reasonable performance and execution time, we only apply this procedure to a portion of the entire dataset (around 10\%).
\todo{correct percentage?}
For further evaluation, we also support different processes for degradation, such as removing more than one method call or degrading more than one type usage.
\todo[inline]{describe degradation a bit better?}

\section{Results \& Interpretation}
% describe results + interpret them with respect to rqs
% A common mistake is to mix the results with the discussion. This makes it harder for the reader to separate backed-up results from speculation.

In the following, we present our results relating to each of the research questions.

\paragraph{For RQ 1: Manual Review}

% concrete techniques and results
\begin{table}[t]
    \centering
    \begin{tabular}[h]{c|l|r|c|c|c|c|c}
\toprule
Nr & Name & Type Usages & Findings & B & S & H & FP \\
\midrule
1 & ar.rulosoft.mimanganu\_78.apk 			& 11241  & 12 & 1 & 1 & 4 & 6   \\
2 & com.zeapo.pwdstore\_94.apk 				& 12659  & 1  &\cc&\cc& 1 & \cc \\
3 & net.bitplane.android.microphone\_7.apk 	& 80     & \cc&\cc&\cc&\cc& \cc \\
4 & com.quaap.dodatheexploda\_2.apk			& 87     & \cc&\cc&\cc&\cc& \cc \\
5 & com.health.openscale\_23.apk 		   	& 3645   &  3 &\cc& 1 & 2 & \cc \\
6 & se.tube42.kidsmem.android\_16.apk 		& 13547  &  1 &\cc&\cc&\cc&  1 \\
7 & org.billthefarmer.diary\_125.apk	   	& 880    & \cc&\cc&\cc&\cc& \cc \\
8 & org.ligi.blexplorer\_12.apk 	    	& 8350   & \cc&\cc&\cc&\cc& \cc \\
9 & org.kaqui\_27.apk				    	& 1675   & \cc&\cc&\cc&\cc& \cc \\
10 & com.afollestad.nocknock\_13.apk     	& 9547   & \cc&\cc&\cc&\cc& \cc \\
\bottomrule
    \end{tabular}
    \caption{The results of the manual evaluation}\label{fig:manual}
\end{table}

In Table \ref{fig:manual}, we summarize the results of manually reviewing the findings of the $\text{DMMC}$ variation.
In the entire dataset, on average there were XYZ anomalous type usages per application, so our sample with $1.7$ findings per app seems like a good fit.
Of the total 17 findings, 3 are true positives (bug or smell) meaning that $17.65\%$ of findings are true findings.
If we also consider hints to be true findings, this number increases to $58.82\%$.
\todo[inline]{
    order of description makes sense like this or not really?
    mention average number of findings per app (remember that the findings now are for all 626 apps) (all findings)
}

Most of the applications do not exhibit any anomalous type usages and those that do only have a few.
There does not seem to be a relationship between the number of type usages and the number of findings per application.
One of the application is responsible for the majority of findings and also for the majority of false positives.
However, it is also the only one in which a finding indicates a true bug.
In fact, we reported this bug and the maintainer already confirmed and fixed it\footnote{\url{https://github.com/raulhaag/MiMangaNu/issues/535}}.

\begin{table}[t]
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}[h]{r|r|c|c|c|c|c|c|c|c|c|c|c}
\toprule
%\cmidrule(l){3-10}
\multicolumn{1}{c}{ } & \multicolumn{6}{|c}{$\text{DMMC}_\text{noContext}$} & \multicolumn{6}{|c}{$\text{DMMC}_\text{class}$} \\
App & Type Usages & Findings & B & S  & H    & FP & Type Usages & Findings & B & S & H & FP \\
\midrule
1 & 11241  & 41 (24) & 1 (1)  & 1 (1) & 20 (15) & 19 (7) & ? & 28 (14)&1 (1)&  \cc &8 (5)& 19 (8) \\
2 & 12659  & 16 (3)  &  \cc   &  \cc  &  \cc    & 16 (3) & ? & 10 (8) & \cc &  \cc &1 (1)&  9 (7) \\
3 & 80     &  0 (0)  &  \cc   &  \cc  &  \cc    &  \cc   & ? &  0 (0) & \cc &  \cc & \cc & \cc \\
4 & 87     &  0 (0)  &  \cc   &  \cc  &  \cc    &  \cc   & ? &  0 (0) & \cc &  \cc & \cc & \cc \\
5 & 3645   &  8 (6)  &  \cc   &  \cc  &  3 (1)  &  5 (5) & ? &  7 (2) & \cc &  \cc & \cc &  7 (2) \\
6 & 13547  & 23 (0)  &  \cc   &  \cc  &  7 (0)  & 16 (0) & ? &  0 (0) & \cc &  \cc & \cc & \cc \\
7 & 880    & 21 (2)  &  \cc   &  \cc  &  \cc    & 21 (2) & ? & 21 (2) & \cc &  \cc & \cc &  21 (2) \\
8 & 8350   &  4 (4)  &  \cc   &  \cc  &  \cc    &  4 (4) & ? &  2 (2) & \cc &  \cc &1 (1)&   1 (1) \\
9 & 1675   &  2 (1)  &  \cc   &  \cc  &  2 (1)  &  \cc   & ? &  1 (1) & \cc &  \cc &1 (1)& \cc \\
10& 9547   & 15 (9)  &  \cc   &  \cc  &  \cc    & 15 (9) & ? &  4 (3) & \cc &  \cc & \cc &   4 (3) \\
\bottomrule
    \end{tabular}}
    \caption{Comparing $\text{DMMC}_\text{noContext}$ and $\text{DMMC}_\text{class}$. The numbers in brackets indicate findings on the project itself.}\label{fig:manual2}
\end{table}
\todo[inline]{
Fill the tu nr for classMerge!!!
consider if there is some better way than the brackets? (tried the / but looks like a fraction\ldots)
}

In Table \ref{fig:manual2}, we present the results of applying $\text{DMMC}_\text{noContext}$ and $\text{DMMC}_\text{class}$ to the same applications.
When ignoring the context, the total number of findings grows to 130 of which only 2 are true positive ($1.54\%$), if we also consider the 32 hints to be true positives, the ratio goes up to $26.15\%$.
Since this are a lot of anomalies, we apply a filter to the results that removes any anomaly that does not originate from a class that belongs to the application itself.
As a developer these are the most interesting anomalies, because they can be fixed immediately.
Other anomalies that originate in libraries or other external code will be much harder to fix as they are usually maintained by different people.
The number of findings that remain after applying this filter are displayed in brackets.
It reduces the total number of findings to 49 of which 2 ($4.08\%$) or 19 ($38.78\%$) are true positives depending if we allow hints or not.

The class merge variation detects a total of 73 anomalies with 1 true positive among them ($1.37\%$).
If we factor in the hints, $16.44\%$ of findings are true positives.
By excluding the type usages that do not originate form the application itself, we reduce the total number of anomalies to 32 of which $28.12\%$ are true positives (including hints).
And to clarify, the bug that all of the variations uncover is always the same.

\paragraph{For RQ 2 and 3: Benchmark}

First, we present the benchmark results of the different variants $\text{DMMC}$, $\text{DMMC}_\text{noContext}$ and $\text{DMMC}_\text{class}$.

\todo[inline]{
present results!
-> first in general, then also in relation to partition size
(maybe relate the precision etc to the number of tus in the partition -> already have that data)
}

\paragraph{For RQ 4: Robustness}

benchmark results for degrading 1 or more ADDITIONAL TUS?!

\paragraph{For RQ 5: Other Anomalies}

\begin{table}[t]
    \centering
    \begin{tabular}[h]{r|l|r|c|c|c|c|c}
\toprule
& & \multicolumn{2}{c|}{$\text{DMMC}_\text{superfluous}$} & \multicolumn{3}{c}{$\text{DMMC}_\text{wrong}$} \\
Nr & Type Usages & Findings & On Project & Findings & >1 Call & On Project \\
\midrule
 1 &  11241  & 51 & 43 & 95 & 18 & 16  \\
 2 &  12659  & 22 & 19 & 46 &  5 &  5  \\
 3 &  80     &  1 &  1 &  1 & \cc& \cc \\
 4 &  87     &  6 &  6 &  8 &  2 &  2  \\
 5 &  3645   & 22 & 20 & 33 &  9 &  9  \\
 6 &  13547  &  8 &  1 &  1 & \cc& \cc \\
 7 &  880    &  4 &  4 &  2 & \cc& \cc \\
 8 &  8350   &  5 &  2 &  2 & \cc& \cc \\
 9 &  1675   &  4 &  4 &  5 &  1 &  1  \\
10 &  9547   & 26 & 11 & 39 &  9 &  9  \\
\bottomrule
    \end{tabular}
    \caption{Anomalies flagged by $\text{DMMC}_\text{superfluous}$ and $\text{DMMC}_\text{wrong}$}\label{fig:other}
\end{table}

The results of applying the two variations $\text{DMMC}_\text{superfluous}$ and $\text{DMMC}_\text{wrong}$ to the randomly selected applications are depicted in Table \ref{fig:other}.
Both methods consider a lot of type usages to be anomalous - 
 METNION NUBMER!
During manual review we did not find a single true positive among them or even a smell or hint, that is, all of these anomalies are false positives.
To put these into proportion, we have again filtered the findings by \ldots

-> mention and explain the >1 call problem!
general flaw of WRONG -> all tus with just one call (on the same type) are considered AE! (even if with context this is too many!)
-> LOOOOTS of noise and super uselless, thus we also filter out these
-> nonethelsse all fps\ldots

\section{Discussion}
% Interpretation of the results that go further than the research questions. This can, e.g., contain implications for software development.

\paragraph{For RQ 1: True vs False Positives}
% how is the quality of findings (RQ1), how many findings are there, how much work is it to evaluate them

% general
The results of the manual review show that some of the anomalous type usages indeed indicate true bugs or smells.
The number of anomalies per app is relatively small, such that a developer could easily evaluate them himself.
While the manual review was time-consuming, this is partially because we do not know the applications, a developer that is familiar with it would be much faster.
Because of the small sample size, we consider the actual percentages of true positives to be of little import.
\todo[inline]{somehwere here say something about general applicability of this method or only in conclusion?}

% mention side study of top 50 anomalies!
Besides manually reviewing the anomalies in 10 random applications, we also performed a side study on the top 50 anomalies of all applications.
Over its course we did not uncover any additional bugs, but instead a lot of false positives.
Most of these false positives arise out of a small number of failure modes such as:
\begin{itemize}
    \item suggesting a missing call to \code{<init>} (basically never valid, usually the object is a parameter or created by a static function)
    \item not following static functions (in which the suggested missing method is called)
    \item detecting not existing patterns with \code{StringBuilder}
    \item problems with method chaining (does not consider the returned object to be the same as before)
\end{itemize}
Some of these could be fixed rather easily, e.g., we could filter all type usages related to \code{StringBuilder} or anomalies that suggest that a call to \code{<init>} is missing.
Such filters would reduce the number of false positives, but they are also slightly antithetical to the initial idea of avoiding hand crafted rules.
Other problems are more difficult to remedy.
Correctly detecting method chaining seems hard or will at least have some trade-offs.
It seems possible to consider static function evaluations, but we expect it to be expensive in terms of performance.

% no context / class merge
As for RQ 1.1, regarding the comparison between the different variations, it seems clear that the variants $\text{DMMC}_\text{noContext}$ and $\text{DMMC}_\text{class}$ are inferior to the normal $\text{DMMC}$ system.
Not only do they flag more anomalies, the anomalies that they discover are also less relevant.
They find a lot of anomalies in libraries and external code, but even after filtering these out, $\text{DMMC}$ is still better.
\todo{fyi the filter only removes 2 FP anomalies form the normal variation}

\todo[inline]{ even mention this? I think not right now\ldots

failure modes: wihtout context: anomalies when clases call functions in itself
many repetitions of identical findings
Kotlin thingy (even mention SC thing?)
-> not many kotlin apps -> easy to be an outlier for some stuff (init for example), etc
failure mode: kotlin -> outlier generally
}

\paragraph{For RQ 2: Benchmark Results}


\paragraph{For RQ 3: Input Size}

% math answer
Again this question is extremely hard to answer with certainty.
However, we would like to present another mathematical argument. 
As we already laid out in the previous section, a type usage needs at least 9 almost similar instances to reach a strangeness score of 0.9 and be flagged as an anomaly.
It follows that there must be at least 10 type usages within the same partition to even have a theoretical chance of finding a bug.
Now consider that the $\text{DMMC}$ variant which does take the context into account has shown the best performance and the distribution of context and type partitions presented in Figure XY.
\todo{add reference and rephrase}
Even in our large dataset of 625 applications, most partitions are still rather small. 
It seems that it is rather difficult to obtain the necessary rich dataset, even if taking a lot of code into consideration.
\todo[inline]{
We also applied our implementation to a medium sized industry application (TS!) (mention LOC) and the results it produced supported this intuition, being of rather poor quality.
any other actual results on this? I don't actually think so\ldots

Siggy:
``maybe 1-2 sentences more where you explain (again) what small datasets mean and why it seems difficult to obtain necessary rich datasets (stated in the next sentence). you could emphasize the size of the analyzed framework here again.''
you could conclude what this means with reference to the question in the title 
}

\paragraph{For RQ 4: Robustness}

\todo[inline]{
example: use true bug finding, try to throw away parts of the data -> when can we not find this anymore -> mathematical answer!
give example about the true bug we found + how fast it would disappear from the findings list? or rather how slow
-> this only in discussion!
}

For the mathematical answer, consider how the strangeness score of a true bug $b$ behaves if we introduce additional erroneous instances.
\todo{rather explain with two bugs b and b'?}
If there are no instances equal to $b$, then $|E(b)|=1$ and $\operatorname{S-score}(b)=1-\frac{1}{1+|A(b)|}$.
If we consider type usages with a strangeness score of more than $0.9$ to be anomalies, this bug would be detected as soon as there are at least $9$ instances that are almost equal.
On the other hand, if there is one instance that is equal to $b$, then $|E(b)|=2$ and thus $\operatorname{S-score}(b)=1-\frac{2}{2+|A(b)|}$.
We would only detect this bug if there were at least $18$ instances that are almost equal to it.
\todo[inline]{
    here or only in discussion?
To understand what this mean consider the graph in Figure X displaying the distribution of partition sizes in our dataset.
Most of them are small and thus having an additional mistake in the dataset can easily prevent our implementation from flagging both as an anomalie.
oö
+ doubles the requirements to the input size oä
}

\paragraph{For RQ 5: Other Anomalies}

just something like: considering the high number of detected findings and zero actual success
+ the patterns that were detected
-> majority rule cannot be considered useful for detecting wrong or superfluous calls
\section{Threats to Validity}

Split into external vs interal threats!

and of course regarding the ``results'' always qualify -> this only for android, open source apps, manual evaluation, blabla, so we never actually know for sure

% internal: reasons why the results could be invalid for your study objects
only paramter: strangeness cutoff -> of course could be set higher

manual eval: biased, subjektive bewertet, but rated rather conversatively at least
since I wanna have a neg conclusion, I can sY that I graded rather friendly with the hints etc => could be rated even worse

general problem of automatic benchmark! (already mentioned but still applies to other questions)
    we can only show that the benchmark is not WRONG, cannot show it's right
how related are the degraded TUs to missing method calls you can find in the wild?
improving the metrics by dropping cases where we know we won't find an answer!!!

% external: reasons why the results encountered for the study objects might not be transferable to other objects.
% In the example, the way we chose the study objects (through our personal network) might bias our results. To mitigate this threat, we at least chose systems that had different characteristics, such as programming language, development contractor and age.
wie sehr sind die resultate aus der Android case study 1. Wahr (subjektive bewertet etc) 2. Übertragbar auf andere Anwendungsfälle(sind open-source programme of vergleichbar mit professionellen, Android eco System mit anderen, etc)

