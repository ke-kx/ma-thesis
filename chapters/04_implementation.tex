\chapter{Implementation}\label{ch:impl}

\todo[inline]{Generally: don't make this section too long I'd say!}

After laying out the theoretical foundations of the majority rule method in the last chapter, this chapter focusses on the practical side.
\todo{rephrase a bit}
We give details about design decisions taken and the reasons for them, pitfalls that had to be overcome and the trade-offs that had to be accepted.
Additionally, we explore some of the mistakes made and dead ends that we encountered.

The first section focusses on the internal workings of the system we developed.
We explain the steps from receiving the input program to outputting a list of anomalies which hint at potentially missing method calls.
The second section revolves around the benchmark system, the basic idea behind it and how it is designed to ensure maximum flexibility.

\section{Procedure}

Our system is (primarily) a system for detecting missing method calls.
It works by statically analyzing a given software application, extracting the type usages which it contains and finally determining if any of them are anomalous by the Majority rule as described in Section \ref{sec:majority}.
The end result should be a list of locations which are potentially missing a method call.
Given a software system, which shall be tested for anomalies, conceptually the following steps have to be realized:

\begin{enumerate}
    \item Extract all type usages from the software
    \item For each type usage $x$ among them:
    \begin{enumerate}
        \item Search for type usages which are exactly similar to $x$ (i.e. calculate $E(x)$)
        \item Search for type usages which are almost similar to $x$ (that is determine $A(x)$)
        \item Calculate the strangeness score of $x$
        \item Extract the list of potentially missing calls and their likelihood $\phi$
    \end{enumerate}
    \item Output a list of anomalous type usages, sorted by their $\operatorname{S-score}$, together with the calls they are potentially missing
\end{enumerate}

\todo{(the ones with a score above XX are considered to be an anomaly)}

%-- description of system
However, this simple outline does not represent the actual realities of the system.
Instead of a singular process with [flat / not branching / \ldots] flow, it is split into three different parts which are laid out in Figure \ref{fig:overview}.
First, a java application reads the byte code of the program under analysis and iterates over all method definitions to extract the type usages which are present.
The extracted type usages are then persisted in a database to ensure flexibility and high performance in the following analysis phase.
For the actual anomaly detection we use a relatively simple python program to enable fast prototyping and again flexibility.
It goes through all sets of type usages (more on those sets in Section \ref{sec:anomaly}), requests the type usages in the set and calculates their strangeness score.
After iterating through all the sets of the application it finally outputs the results.

%summary of rest
The following sections give a more in depth explanation of the separate steps.
\todo{smth more? + move figure ABOVE the paragraph explaining it?}

\begin{figure}[h]
\centering
\begin{tikzpicture}
    \tikzset{vertex/.style={draw,rounded corners,align=center}}
    \tikzset{edge/.style = {->,> = latex'}}

    % start dot
    \node[fill=white] (startTxt) {Application};
    \node[fill=black,circle,inner sep=2pt] (start) [below = 0.25cm of startTxt] {};

    % java + db
    \node[vertex] (j1) [right = 1.5cm of start] {Read\\bytecode};
    \node[vertex] (j2) [right = 0.5cm of j1] {Extract\\type usages};
    \node[vertex] (db1) [right = 0.5cm of j2] {Persist\\type usages};

    \draw[edge] (start) to (j1);
    \draw[edge] (j1) to (j2);
    \draw[edge] (j2) to (db1);

    % py + db + end
    \node[vertex] (db2) [below = 0.25cm of db1] {Answer\\query};
    \node[vertex] (py1) [below left = 0.5cm of db2] {Request\\current set};
    \node[vertex] (py2) [below right = 0.5cm of db2] {Calculate\\ $\operatorname{S-score}$};
    \node[fill=black,circle,inner sep=2pt] (end) [right = 1.5cm of py2] {};
    \node[fill=white,align=center] (endTxt) [above = 0.25cm of end] {List of\\anomalies};

    \draw[edge] (py1) to (db2);
    \draw[edge] (db2) to (py2);
    \draw[edge] (py2) to (end);

    % big dashed boxes
    \node[label=above left:{Java}, draw=black, thick, dashed, inner sep=0.25em, fit=(j1) (j2)] {};
    \node[label=right:{Database}, draw=black, thick, dashed, inner sep=0.25em, fit=(db1) (db2)] {};
    \node[label=left:{Python}, draw=black, thick, dashed, inner sep=0.25em, fit=(py1) (py2)] {};

\end{tikzpicture}
\caption{System overview}
\label{fig:overview}
\end{figure}
\todo{make a bit nicer (alles bisi auseinander ziehen?), better position dash box labels | POTENTIALLY add the retrieve sets step to python?}
% https://tex.stackexchange.com/questions/58878/tikz-set-node-label-position-more-precisely

\subsection{Bytecode Analysis}\label{sec:bytecode}
\todo[inline]{maybe rename section?}
somewhere: if necessary: compile program, then analyze!

%-- general intro
soot as analysis framework -> what is soot
-> reference: \url{https://sable.github.io/soot/}
originally java optimization framework
wide range of usages, analyze, instrument, optimize and visualize java (and by extension android) applications
provides call graph constructions, points-to analysis, def/use chains, inter and intra-procedural data-flow analysis,
taint analysis
we will use it for XY

%-- why bytecode over sourcecode
operates by statically analyzing a piece of software
(not on the source code, but rather on the compiled byte code -> why: easier)
in theory soot would support sourcecode analysis(?), but actually the bytecode analysis is recommended (source?)
easier for experiments
more standardized (jvm byte code)
[borrow smth from this one paper (don't remember which one\ldots)]

%-- some more in depth info on how soot does what it does?
transforms given programs into intermediate representation on which it operates
there are four immediate representations (depending on use case)
we use Jimple, the primary representation, typed 3-address intermediate rperesnetation (especially suitable for optimization, but also for our zweck)

%-- soot startup and settings?
soot is very powerful and one can customize it to carter to your specialized needs
Mostly we will analyze android apps and the settings we are using reflect this
-> table with settings + explanation?

\begin{figure}[h]
    \centering
    \begin{tabular}[h]{c|c|c}
    Option & Parameter & Explanation \\ \hline
    \code{-app } & - & Run in application mode \\ \hline
    \code{-keep-line-number} & - & Keep line number tables \\ \hline
    \code{-output-format} & none & Set output format for Soot \\ \hline
    \code{-allow-phantom-refs } & - & Allow unresolved classes \\ \hline
    \code{-src-prec} & apk-class-jimple & Sets source precedence to format files \\ \hline
    \code{-process-multiple-dex} & - & Process all DEX files found in APK \\ \hline
    \code{-android-jars} & [path] & The path for finding the android.jar file \\ \hline
    \end{tabular}
    \label{fig:sootparam}
    \caption{The parameters passed to Soot}
\end{figure}

%-- general / not sure where yet
explain the changes I made to their code

\subsection{Extracting Type Usages}

some more on extraction! + the algorithm!
some information on what kind of further analysis is attempted (local must alias, bla)

%-- general ablauf
where is it registered in soot and what does that mean?
soots execution divided into a number of phases
each phase has a specific task like for example the aggregation of local variables
Phases are further grouped into Packs and the registered packs are applied succesevely during the analysis
we can register custom parts which will apply some work in the specified phase
register custom \code{BodyTransformer} with soot.
in Pack: jtp -> stands for Jimple Transformation Pack  and is simply applied to every method under analysis (unmodified soot has no transformations in this pack)

%--- what is the transformer doing?
generally: applied to each method body
first iterates through all statements and checks if they are a method call, if so remember them (aka extract list of method calls happening in this method)
given this list of method call, iterate through it and group them to type usages
this is done by checking first on which VARIABLE the call was made
additionally, check if other locals point to the same underlying object -> group the calls made on these locals into one tu
then group the calls which were made on the same object and be happy
(more details necessary? I don't really think so)

%-- local must alias analysis
% https://soot-build.cs.uni-paderborn.de/public/origin/develop/soot/soot-develop/jdoc/soot/jimple/toolkits/pointer/LocalMustAliasAnalysis.html
inaccuracy about ``object'' vs ``variable'' of a particular type?
``LocalMustAliasAnalysis attempts to determine if two local variables (at two potentially different program points) must point to the same object. The underlying abstraction is based on global value numbering'' -> this is a soot feature
checking if two locals point to the same object can be expensive + seems a little bit leaky
in test runs on big applications this has let memory requirements explode, thus we made it optional
however for android was fine (even mention this?)

%-- ignoring some classes
for android we chose to exclude some classes from analysis (do not step into them!, still record type usages!)
because not the real code -> not really interested 
also performance reasons (quite important actualy)
in theory soot has a mechanism for this, but it isn't working correctly
packages thusly excluded are:
\code{java.*}, \code{sun.*}, \code{android.*}, \code{org.apache.*}, \code{org.eclipse.*}, \code{soot.*}, \code{javax.*}
(probably only mention java, javax and android?)

\subsection{Storing Type Usages}

%intro / übergang + general info
after the transformer has analyzed and extracted a list of type usages form the current method body, they are persisted into a database
using hsqldb -> link
SQL relational database written in Java
Provides small, multithreaded and transactional database engine
in memory and disk based tables 
reasons for using this one?
easy setup, small, fast
permissive license, lots of features
+ we also invested some time into exploring the posibility to use postgres, however it turned out that the performance was worse -> see section (dead ends)

%- why are we using a database at all?
monperrus et all saved data in a text based format
however there are a couple of disadvantages to this
-> need to parse again for analysis, slow, large data storage
using a database has many positive ramifications
flexibility, future extensibility, easier to prototype fast (use python for analyis eg)
queries! -> much better, need not hold everything in memory, especially for bigger datasets, can access data selectively
also speed

somewhere(here?) mention, that it is not only possible but even SMART to not only incorporate the type usages of ONE application into the analysis but if at all possible, all of the software which uses the same kind of framework/library
+ explain why obviously
    of course it's useful to ALSO analyse tus over classes which are private to my application (if there is enough data)
    however, mostly interesting are the framework classes which are used by many applications
    when analyzing the usages of framework etc classes, we want to have as much information as possible

%-- database layout + some of the considerations made (x. normalenform, sowas?)
-> some sort of sql graphic? -> use the one I get from Intellij!
+ describe obviously (+ what was made how why)

\subsection{Anomaly Detection}\label{sec:anomaly}
only part affected by the different variants presented in chapter 3
retrieving relevant type usages for the current variant + potentially modifying them + throwing them into the detector
-> explain a bit the python class structure I developed?

maybe another img or flow chart for python ablauf (or too much detail?)o

somewhere explain the ``sets of tus'' + peformance issues before (ref db dead end)

\subsection{Improvements and Dead Ends}
\todo{should this be a section or a subsection?}

explaining all the work i did + what was already there
    first reading their code, coming across a couple of discrepancies between code and paper, later check if everthing still works (evaluation)
    refactoring everything + saving stuff to database
    building python infrastructure for analysis

evaluation of other db system + why it didn't work (basically pure db stupidity)

some changes that had to be made to the analysis framework for android analysis

why some solutions where discarded (eg pure database / could be revisited if it turns out to be the best anyways - performance)
Better anomaly detection (is the anomality rule actually a GOOD measure for this kind of anomalies, or should we use something totally different?))
    clustering detector try (+ hypersphere idea?)

static functions evaluation!
something to fix the dotchaining problems

\section{Benchmark}
some more subsections? (different ways for degrading type usages?)

this only to be about the automatic benchmark stuff!

building benchmarking infrastructure
flexibility with python class setup
