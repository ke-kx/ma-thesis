\chapter{Evaluation}
general idea behind evaluation method and what we are trying to detect
actually it's a sort of simulation (see recommender system book p. 301f) - micro vs macro evaluation, \ldots
    imitation of the real system of software development
    using a much simpler system, namely dropping method calls
    obvious question: how similar are the such created mmcs to mmcs in the wild?
Robustness explanation
Metrics explanation - Precision, Recall
    additional metrics from recommender systems book p.245f (robustness, learning rate - p. 261)
    performance -> training time + performance when working


checking if their ``mistakes'' / inaccuracies make a difference or not

(Case Study - if time...)
    ANDROID?!
    would be super sexy to apply the best method to some library / framework and see what happens

subsection: threats to validity
