\chapter{Implementation}

\todo[inline]{Generally: don't make this section too long I'd say!}

after laying out the theoretical foundation of the dmmc method we examine (in this thesis) in the last chatper
this chapter focusses on the implementation side
details about decisions taken, pitfalls that had to be overcome and tradeoffs to be made
and of course things which didn't work out

the first section focusses on the internal workings of the system we developed.
explain the steps from program (code) to list of potential anomalies
smth smth
The second one revolves around the benchmark system, the basic idea behind it and how it is designed to ensure maximum flexibility

\section{Procedure}

our system is (primarily) a system for detecting missing method calls
it works by statically analyzing some software application, extracting the type usages which are present
and finally determining if any of them is anomalous by the Majority rule as described in Section + ref
The end result should be a list of places which are potentially missing a method call

Given a software system, which shall be tested for missing method calls, 
conceptually this needs to be done:

\begin{enumerate}
    \item Extract all type usages from the software
    \item For each type usage $x$ among them:
    \begin{enumerate}
        \item Search for type usages which are exactly similar to $x$ (i.e. calculate $E(x)$)
        \item Search for type usages which are almost similar to $x$ (that is determine $A(x)$)
        \item Calculate the strangeness score of $x$
        \item Extract the list of potentially missing calls and their likelihood $\phi$
    \end{enumerate}
    \item Output a list of anomalous type usages, sorted by their $\operatorname{S-score}$, together with the calls they are potentially missing
\end{enumerate}

(the ones with a score above XX are considered to be an anomaly)

%-- why this simple outline is not actually enough

using a database + flexible python benchmarking / analysis
-> proper overview of resulting system: java analysis -> db -> python
-> somewhere a proper overview of the concrete steps that are taken + explanation of them?! - similar to monperrus2010 p7

%-- description of system
these considerations give rise to the system design which is displayed in figure + ref
actual practical implementation:
first a java applications reads the byte code of the application and then iterates over all method definitions to extract the type usages which are present
those type usages are then persisted in a database
the actual anomaly detection is handled by relatively simple python program
it requests the currently relevant set of tus (more on that in +ref) and calculates their strangeness score
after iterating through all sets of an application it can output the results

%-- image of system
nice image:
java part - db part - python part
1. bytecode einlesen + type usages extracten
2. type usages abspeichern und nach anfrage rausgeben
3. tus anfragen und sets ausrechnen

\begin{figure}[h]
\centering
\begin{tikzpicture}
    \tikzset{vertex/.style={draw,rounded corners,align=center}}
    \tikzset{edge/.style = {->,> = latex'}}

    % start dot
    \node[fill=white] (startTxt) {Application};
    \node[fill=black,circle,inner sep=2pt] (start) [below = 0.25cm of startTxt] {};

    % java + db
    \node[vertex] (j1) [right = 1.5cm of start] {Read\\bytecode};
    \node[vertex] (j2) [right = 0.5cm of j1] {Extract\\type usages};
    \node[vertex] (db1) [right = 0.5cm of j2] {Persist\\type usages};

    \draw[edge] (start) to (j1);
    \draw[edge] (j1) to (j2);
    \draw[edge] (j2) to (db1);

    % py + db + end
    \node[vertex] (db2) [below = 0.25cm of db1] {Answer\\query};
    \node[vertex] (py1) [below left = 0.5cm of db2] {Request\\current set};
    \node[vertex] (py2) [below right = 0.5cm of db2] {Calculate\\ $\operatorname{S-score}$};
    \node[fill=black,circle,inner sep=2pt] (end) [right = 1.5cm of py2] {};
    \node[fill=white,align=center] (endTxt) [above = 0.25cm of end] {List of\\anomalies};

    \draw[edge] (py1) to (db2);
    \draw[edge] (db2) to (py2);
    \draw[edge] (py2) to (end);

    % big dashed boxes
    \node[label=above left:{Java}, draw=black, thick, dashed, inner sep=0.25em, fit=(j1) (j2)] {};
    \node[label=right:{Database}, draw=black, thick, dashed, inner sep=0.25em, fit=(db1) (db2)] {};
    \node[label=left:{Python}, draw=black, thick, dashed, inner sep=0.25em, fit=(py1) (py2)] {};

\end{tikzpicture}
\caption{System overview}
\label{fig:overview}
\end{figure}
\todo{make a bit nicer (alles bisi auseinander ziehen?), better position dash box labels}
% https://tex.stackexchange.com/questions/58878/tikz-set-node-label-position-more-precisely

%summary of rest
The following sections give more in depth information on the separate steps.
smth mroe?

\subsection{Bytecode Analysis}\label{sec:bytecode}
maybe rename section?
somewhere: if necessary: compile program, then analyze!

%-- general intro
soot as analysis framework -> what is soot
-> reference: \url{https://sable.github.io/soot/}
originally java optimization framework
wide range of usages, analyze, instrument, optimize and visualize java (and by extension android) applications
provides call graph constructions, points-to analysis, def/use chains, inter and intra-procedural data-flow analysis,
taint analysis
we will use it for XY

%-- why bytecode over sourcecode
operates by statically analyzing a piece of software
(not on the source code, but rather on the compiled byte code -> why: easier)
in theory soot would support sourcecode analysis(?), but actually the bytecode analysis is recommended (source?)
easier for experiments
more standardized (jvm byte code)
[borrow smth from this one paper (don't remember which one\ldots)]

%-- some more in depth info on how soot does what it does?
transforms given programs into intermediate representation on which it operates
there are four immediate representations (depending on use case)
we use Jimple, the primary representation, typed 3-address intermediate rperesnetation (especially suitable for optimization, but also for our zweck)

%-- soot startup and settings?
soot is very powerful and one can customize it to carter to your specialized needs
Mostly we will analyze android apps and the settings we are using reflect this
-> table with settings + explanation?
\begin{tabular}[h]{c|c|c}
Option & Parameter(s) & Explanation \\ \hline
\code{-app } & - & Run in application mode \\ \hline
\code{-keep-line-number} & - & Keep line number tables \\ \hline
\code{-output-format} & none & Set output format for Soot \\ \hline
\code{-allow-phantom-refs } & - & Allow unresolved classes \\ \hline
\code{-src-prec} & apk-c-j (apk-class-jimple) & Sets source precedence to format files \\ \hline
\code{-process-multiple-dex} & - & Process all DEX files found in APK \\ \hline
\code{-android-jars} & [path] & Use \textit{path} as the path for finding the android.jar file \\ \hline
\end{tabular}

%-- general / not sure where yet
explain the changes I made to their code

\subsection{Extracting Type Usages}

some more on extraction! + the algorithm!
some information on what kind of further analysis is attempted (local must alias, bla)

%-- general ablauf
where is it registered in soot and what does that mean?
soots execution divided into a number of phases
each phase has a specific task like for example the aggregation of local variables
Phases are further grouped into Packs and the registered packs are applied succesevely during the analysis
we can register custom parts which will apply some work in the specified phase
register custom \code{BodyTransformer} with soot.
in Pack: jtp -> stands for Jimple Transformation Pack  and is simply applied to every method under analysis (unmodified soot has no transformations in this pack)

%--- what is the transformer doing?
generally: applied to each method body
first iterates through all statements and checks if they are a method call, if so remember them (aka extract list of method calls happening in this method)
given this list of method call, iterate through it and group them to type usages
this is done by checking first on which VARIABLE the call was made
additionally, check if other locals point to the same underlying object -> group the calls made on these locals into one tu
then group the calls which were made on the same object and be happy
(more details necessary? I don't really think so)

%-- local must alias analysis
% https://soot-build.cs.uni-paderborn.de/public/origin/develop/soot/soot-develop/jdoc/soot/jimple/toolkits/pointer/LocalMustAliasAnalysis.html
inaccuracy about ``object'' vs ``variable'' of a particular type?
``LocalMustAliasAnalysis attempts to determine if two local variables (at two potentially different program points) must point to the same object. The underlying abstraction is based on global value numbering'' -> this is a soot feature
checking if two locals point to the same object can be expensive + seems a little bit leaky
in test runs on big applications this has let memory requirements explode, thus we made it optional
however for android was fine (even mention this?)

%-- ignoring some classes
for android we chose to exclude some classes from analysis (do not step into them!, still record type usages!)
because not the real code -> not really interested 
also performance reasons (quite important actualy)
in theory soot has a mechanism for this, but it isn't working correctly
packages thusly excluded are:
\code{java.*}, \code{sun.*}, \code{android.*}, \code{org.apache.*}, \code{org.eclipse.*}, \code{soot.*}, \code{javax.*}
(probably only mention java, javax and android?)

\subsection{Storing Type Usages}

%intro / Ã¼bergang + general info
after the transformer has analyzed and extracted a list of type usages form the current method body, they are persisted into a database
using hsqldb -> link
SQL relational database written in Java
Provides small, multithreaded and transactional database engine
in memory and disk based tables 
reasons for using this one?
easy setup, small, fast
permissive license, lots of features
+ we also invested some time into exploring the posibility to use postgres, however it turned out that the performance was worse -> see section (dead ends)

%- why are we using a database at all?
monperrus et all saved data in a text based format
however there are a couple of disadvantages to this
-> need to parse again for analysis, slow, large data storage
using a database has many positive ramifications
flexibility, future extensibility, easier to prototype fast (use python for analyis eg)
queries! -> much better, need not hold everything in memory, especially for bigger datasets, can access data selectively
also speed

somewhere(here?) mention, that it is not only possible but even SMART to not only incorporate the type usages of ONE application into the analysis but if at all possible, all of the software which uses the same kind of framework/library
+ explain why obviously
    of course it's useful to ALSO analyse tus over classes which are private to my application (if there is enough data)
    however, mostly interesting are the framework classes which are used by many applications
    when analyzing the usages of framework etc classes, we want to have as much information as possible

%-- database layout + some of the considerations made (x. normalenform, sowas?)
-> some sort of sql graphic? -> use the one I get from Intellij!
+ describe obviously (+ what was made how why)

\subsection{Anomaly Detection}
only part affected by the different variants presented in chapter 3
retrieving relevant type usages for the current variant + potentially modifying them + throwing them into the detector
-> explain a bit the python class structure I developed?

maybe another img or flow chart for python ablauf (or too much detail?)

\subsection{Improvements and Dead Ends}
\todo{should this be a section or a subsection?}

explaining all the work i did + what was already there
    first reading their code, coming across a couple of discrepancies between code and paper, later check if everthing still works (evaluation)
    refactoring everything + saving stuff to database
    building python infrastructure for analysis

evaluation of other db system + why it didn't work (basically pure db stupidity)

some changes that had to be made to the analysis framework for android analysis

why some solutions where discarded (eg pure database / could be revisited if it turns out to be the best anyways - performance)
Better anomaly detection (is the anomality rule actually a GOOD measure for this kind of anomalies, or should we use something totally different?))
    clustering detector try (+ hypersphere idea?)

static functions evaluation!
something to fix the dotchaining problems

\section{Benchmark}
some more subsections? (different ways for degrading type usages?)

this only to be about the automatic benchmark stuff!

building benchmarking infrastructure
flexibility with python class setup
