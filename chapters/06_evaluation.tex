\chapter{Evaluation}\label{ch:eval}

\todo[inline]{
goal of evaluation: große frage: bringt das Verfahren etwas?
understand how well this method works in the context of the android ecosystem, and the NATURE of findings

does this need something more?
/ point out the threats to validity / problems / improvement / smth?
}

% Which questions are we trying to answer with the evaluation?
Our evaluation of the method proposed by Monperrus et al.\ is aimed at comprehending how well the majority rule is suited to detect missing method calls in object oriented software.
We would like to understand better the nature and quality of findings produced by our implementation.
To achieve this, we apply our implementation to a large dataset of Android applications and evaluate the results qualitatively and quantitatively.
% chapter outline
In the following, we first give an overview of our methodology before we examine the dataset.
We present some general data and analyze how type usages behave in Android applications.
We discuss the results of the evaluation in response to 5 research questions which are all aimed at shedding some light on the applicability of this method.
Finally, we consider the meaning of these results and what might threaten their validity.

\section{Methodology}

% The introduction to your methodology section should begin by restating the research problem and underlying assumptions underpinning your study
In this thesis we explore an approach for detecting missing method calls.
We present an implementation of the method proposed by Monperrus et al.\ as well as two other related variations.
The goal of the evaluation is to understand how they behave and what kind of results one can expect.
\todo{more?}
% so what we are doing
Our experimental evaluation is twofold: a qualitative evaluation of the findings in 10 randomly selected Android apps and a simulation experiment using artificially created errors.
For the qualitative evaluation, we manually check each anomalous type usage and flag it as a true or false positive.
For the simulation, we remove method calls from the code and checking if our implementation can detect this known missing method call.
Since it is not assured that the results of this simulation correspond to the real performance, we also attempt to evaluate the applicability of the simulation experiment.
\todo{maybe a bit nicer / more clear / less information?}

\subsection{Qualitative Evaluation / Android Case Study}
\todo[inline]{decide on title\ldots}

% general description + source of android apps
F-droid\footnote{\url{https://f-droid.org/en/}} is a repository for Free and Open Source software (FOSS) on the Android platform.
We used a scraper to download the latest version of all available applications on the 6th of March 2018.
From these 625 Android applications, we extract all type usages that are present and apply our implementation of the $\text{DMMC}$ system to identify any anomalies.

% idea behind using android
The Android ecosystem is particularly suited for this evaluation and enables us to evaluate how type usages and the majority rule behave on real software.
The Java code is easy to analyze and there is a rich open source community, placing many sample applications at our disposal.
Each of these applications uses the Android API, so that we can generate a large dataset on the classes in the API, and detect any common patterns that emerge when using it.
Because of the API's emphasis on the user interface, we consider this to be an environment in which patterns will appear most frequently and most clearly.
\todo{formulate last sentence a bit different}

% actual steps to evaluation
\todo[inline]{ past tense makes sense or not really?  }
The detailed procedure of the manual evaluation was as follows:
We extracted the type usages of the 625 Android applications that we had downloaded and stored them in the database.
Next, we analyzed the extracted type usages with the 3 different variants $\text{DMMC}$, $\text{DMMC}_{\text{noContext}}$ and $\text{DMMC}_{\text{class}}$ and determined their respective $\operatorname{S-score}$s.
We consider type usages with an $\operatorname{S-score}$ bigger than $0.9$ to be anomalies.
This is the value that Monperrus et al.\ suggest and since it implies that 90\% of similar type usages are almost similar it seems like a reasonable cutoff.

While we would have liked to review all anomalies that our implementation uncovers, there were simply too many.
\todo{determine how many anomalous findings we have in total (easy!)}
Instead, we used a random number generator\footnote{Google frontpage on: \url{https://www.google.com/search?q=random+number}} to sample 10 applications to be examined in detail.
We manually reviewed all anomalous type usages of the selected applications and carefully considered the source code and the API documentation before classifying them as one of the following:
\begin{description}
    \item [real bug (B)] a real defect, change definitely necessary
    \item [real smell (S)] another way of doing things would be better, but probably not a defect in the given instance
    \item [hint (H)] the code is fine, but the pattern is also legitimate; some connection between the methods
    \item [false positive (FP)] the usage is totally fine; no causal relation implying the ``missing'' method must be called
\end{description}

\todo[inline]{
    descriptions fine like this?, don't need the ones below?
\item [Special case(?) (SC)] "weird" implementation with extra comment, for compatibility reasons, something like that
\item [not clear (NC)] source code not available, ...
}

Depending on the quality of results one expects, hints could either be counted towards smells or towards false positives.
We classify type usages as hints when it makes sense that the pattern exists, because many people will use the methods together and there is some causal connection, but, strictly speaking, there is nothing wrong with in the code in question.
As an example consider the situation in which a developer calls \code{next} on an iterator that he just obtained from a list of size one.
Here, he knows that the iterator contains one element so he does not need to check for it with \code{hasNext}, nonetheless, it is useful to detect locations where \code{next} is invoked without a preceding \code{hasNext}.

\todo[inline]{
a bit more explanation?
    proper example code for better understanding?

additional examples:
only height() or width()
setTitle not called -> not strictly necessary but ``makes sense''
}

\subsection{Automated Benchmark}

% general description what its doing + reasons for doing it
We would like to understand better how the majority rule behaves under different circumstances.
During the automated benchmark, we select a type usage from the dataset and remove one of its method calls.
We then calculate the strangeness score of the degraded type usage and determine if the majority rule can detect the known missing call.
As such, we are essentially simulating the situation that a developer forgot the method call in question and verify if our implementation would be helpful in this case.
This procedure enables us to test the system on many cases for that we know the correct answer.
We can calculate relevant metrics that capture the systems success and compare the different variants on an objective basis.

% actual setup + degradation mechanism
We built a flexible benchmarking infrastructure in Python.
It randomly chooses type usages from the dataset and applies one of several degradation mechanisms to it.
The normal mechanism removes a method call and then checks if our implementation detects it.
However, for further evaluation we also support different processes for degradation, such as removing more than one method call or degrading more than one type usage.
\todo{not too much repeated what i said before? + better description of other degradation mechanisms?}

% how do we evaluate stuff
To compare the results of the different variants, we calculate a number of metrics.
\todo[inline]{WHICH metrics do i actually wanna compute?!}
Metrics explanation - Precision, Recall
    additional metrics from recommender systems book p.245f (robustness, learning rate - p. 261)
    performance -> training time + performance when working


\section{Dataset Overview}

\todo[inline]{
% general idea
general data about avg tus related to different settings etc -> what does our dataset even consist of?
mention somewhere: the monperrus2013 p11 highlight? (at least equivalent for my dataset)
Does the Strangeness Score behave as expected on Android Apps? (the behavior that I'm expecting is basically a mathematical necessity!)
    do the general assumptions hold? (most tus have a low score, most apps have few findings, etc)
        -> most tus are ``normal'', a few are ``abnormal''
        what do those assumptions mean? -> they expect some kind of uniformity to the type usages, probably mostly present in GUI etc frameworks

% motivation
concrete questions to answer + way of presenting it
should answer:
    1 What kind of dataset do we have?
    2 How are type usages in Android apps like?
    3 How does the strangeness score behave?
    4 Is it even fast enough? (this here or as separate research question?)
-> get general feeling for this method and the dataset
look into and understand the type of data i have -> how often small inputs, how often big, ...
}

% general dataset
%    How many Apps + How many TUs in total + each (avg / median) -> how big are the apps, (how are the tus split between the apps)
Our dataset consists of 3,880,556 type usages that our Java program extracted from the 625 Android applications downloaded from F-droid.
On average there are X type usages per application and the median is Y.
In Figure X we have plotted the number of type usages per application.
\todo[inline]{Plot TUS / applications}
Most applications are rather small with a few outliers responsible for most of the type usages.
% type usages:
%    percentage on Android framework, percentage on other stuff
Of all the type usages in the dataset, $7.66\%$ operate on types from the Android framework (i.e., their fully qualified package name starts with \code{android.*}).
%    length of method list -> histogram?
In Figure Z we have plotted how many method calls there are per type usage to give a feeling for the amount of information they provide.
\todo[inline]{Plot method list lenghts}

%   number of partitions (types + context / types) + maybe verteilung of number of tus per partition (that seems like an interesting / important one!)
In the variation $\text{DMMC}$, a partition is a valid combination of type in context and there are 1,410,709 combinations in the dataset.
In the variant $\text{DMMC}_{text{noContext}}$, the number of partitions is equal to the number of types and the whole dataset includes a total of 202,988 different types.
Since $\text{DMMC}_{text{class}}$ relies on $A'_\text{noContext}$, the partitions for it are the same.
However, because we merge the type usages before analyzing them, the total number of type usages for this variant goes down to XYZ.
\todo[inline]{determine class merge no of tus (can just load results)}
In Figure X we have plotted the distribution of type usages per partition for the different variants.
Observer that\ldots 
\todo[inline]{Plot verteilung of TUs per partition!, depending on variant}

% performance: total time +
We perform all our experiments on a MacBook Pro with a Intel\textregistered Core™ i7-3720QM CPU @ 2.60GHz and 16GB of RAM.
Extracting the type usages from all applications took a total of 3 hours and 27 minutes, which is an average of around 20 seconds per application.
\todo[inline]{
plot duration of tu extraction in relation to number tus (proxy for project size)
mention somewhere that analysis itself is relatively cheap (especially for new project coming in)
}

\todo[inline]{
strangeness score:
    verteilung of strangeness score (histogram)
    graphs related to general score verteilung
    explain why we expect the scores verteilung to be like that
    score in relation to smth? ie method length, nr tus in partition, etc
    put this only in rq1?  -> not so sure\ldots
}

\section{Results}

\todo[inline]{
general idea:
qualitative evaluation: are the findings useful in practice?
quantitative evaluation: given some assumptions, how useful can we expect this technique to be? robustness? Qualitative vs automatic same results?
make sure I will be able to a) answer those questions and b) the answers are ``interesting''
}

We investigate the performance of our implementation along the lines of five research questions.
The goal of this investigation is to understand how useful we can expect this technique to be in practice.
We would like to understand some of the preconditions it requires and under which circumstances it might fail.
For each question, we first explain its purpose and modus of evaluation, before giving results and an interpretation of them.

\subsection{RQ1: How many true versus false Positives are among the anomalous Type Usages flagged by the Majority Rule?}

% motivation behind question + general
It is among the primary goal of our evaluation to understand if the method we implemented is something that a developer would use to help with maintenance.
It seems that the deciding factor for this would be the ability of the majority rule to detect bugs and to detect them with clarity.
If the developer needs to sift through thousands of anomalies to detect a single mistake, this technique is not very useful.
If on the other hand most of the high ranked anomalies are indeed hints for potential bugs, it would be of tremendous worth.

To mimic the experience a developer would have when applying our tool to his software, we randomly select 10 applications for in detail analysis.
Of these apps we examine each of the anomalous type usages in detail and classify them as described above.
Our measure of success shall be the number of findings per app and the ratio of true to false positive findings.
\todo{mention only here that we do the random sampling instead of above?}

% concrete techniques and results
\todo[inline]{
    prepare table with findings per app (+ avg/median) + mention nr of total TUS!
    only the results of manual evaluation of the 10 random apps with the context variant
+ add the android filter (this will only reduce the number of findings, but maybe improve the quality?!)
+ add the ``only from my project'' filter
}

%maybe:
muster: if there is "several" equal ones it doesn't work so well, if its just very few (<5) it's at least a smell?
qualitatively describe some of the failings?! -> not many kotlin apps -> easy to be an outlier for some stuff (init for example), etc
consider filtering for usages that occur inside the app (by package) to exclude libraries (I think it makes sense...)

potentially have the examples in the discussion rather than here?
example: StringBuilder -> often fails for REASons, but can be easily filtered -> reduce nr of fps
-> these filters also as my ``contribution''? :D -> rather say that it would be kinda backwards from the initial idea to stay away from hand crafted rules
failure mode: doesn't follow static functions (where the method in question is actually called)
failure mode: kotlin -> outlier generally
suggested improvement: missing <init> basically never valid, usually static function, kotlin oä -> remove?
dot chaining -> hard to fix / tradeoffs

mention the android.* filter! / does it actually make sense -> data says not really, rather filter on own app\ldots + that only in the end I'd say

\subsubsection{RQ1.1: Is there a noticeable difference between the different variants?}

do the manual eval each for the slightly adapted forms (no context, class merged) and see if there is any difference
+ include the android filter

%\subsubsection{RQ1.2: Given a known missing method call bug, can this approach detect it?}
% only if time! -> probably not
% reverse test: find actual missing call in bug database -> can I find it using the approach and is it among the top findings?

\subsection{RQ2: Do the Benchmark Results align with the Results of the Manual Evaluation?}

% rough order + stichwörter below\ldots
monp bases lots of their evaluation on the automated benchmark
    This type of simulation is at the heart of the evaluation by Monperrus et al.
the simulation seems to make sense on the face of it
    how 'meaningful' are the benchmarking results?
    does it make sense to evaluate this method using the benchmarking as proposed by Monperrus et al.?
but thinking about it a bit more critically, it's not clear how close the created tus are to real bugs
    unclear how good of a simulation
    While it seems like a reasonable procedure to perform, it relies on a crucial assumption: the artificially created missing method calls are comparable to missing method call bugs in real software.
    this is not inheritly clear to us which is why we actually wanna check this?
    without this assumption the numbers produced by the procedure are meaninglesss
    obvious question: how similar are the such created mmcs to mmcs in the wild?
so in the end the only thing we get are numbers which explain how well the method performs on this artificially designed problem

because of this we would like to present the results of the benchmark twofold:
first just plainly present them + what are we presenting?
    first present benchmark results of different techniques (context, class merge, \ldots) then relate to qualitative results
% how we want to evaluate the simulation itself
second put them in relation to the results of the manual eval -> do we get similar performance between the variants?
    compare results to the results of manual evaluation between the different methods (ie does one perform much worse in both or just in one?)
    are the results better if we leave out the context / merge on a per class basis / FOR DIFFERENT Ks!\ldots
    -> comparison between manual evaluation of findings vs automatic benchmark over the different techniques used (context, no context, class merge, \ldots)

% techniques + results
potentially simulate the benchmark only with tus from exactly the same 10 random apps? (so i don't have to do the whole dataset?)
simply give metrics (precision, recall) for the different variants and relate them to the manual results
(maybe relate the precision etc to the number of tus in the partition -> already have that data)

\subsection{RQ3: How robust is this technique in the face of erroneous input data?}

% general
I would add this, even if the results of comparison with manual evaluation are relatively negative / don't say anything
worauf is robustness bezogen? -> results quality or smth else? (Robustness explanation)

% motivation
it is important to understand what other requirements this method has
here we would like to understand how sensitive the method is to perturbed inputs
the scenario we envision is the following
    this method learns patterns from the code at hand
    given a new and crazy library:
    wie viel brauche ich um sinnvoll viel zu lernen - hälfte der leute machen fehler -> wie robust brauchst du die daten / ist das verfahren !
    what is needed for it to work (lines of code, feature richness, etc) 

problem: how to evaluate when it is "working" vs not working -> again simulate "missing methods", compare performance?
-> mathematical answer at least: how does the strangeness score behave if there are additional erroneous instances
    -> the thing regarding if the same method is removed score halfs / if it's another one it's not so bad (should have written down somewhere right?)
    + probably ppl make a similar mistake assuming it's because this api is hard to use oä
    -> for this it might be hard

% techniques + results
\todo[inline]{how to best present the robustness results?}
I mean I have the benchmark test, so i might as well do it, but also mention mathematical results!
for true findings, try to throw away parts of the data -> when can we not find this anymore -> mathematical answer!
/ instead of throwing away: introducing random errors in the related tus

definitely also mention some MATH
    if there is only one anomaly and we get another one (of the same type) can half the strangeness score
    or just decrease it in proportion to the almost equal number (if wrong method gets removed)
    but assumption: if one method is difficult, it's most likely the one ppl forget
but also benchmark!
try with intentionally degraded data -> I already have the setup, I just have to improve the benchmark thing performancewise a bit and let it run
also: for true findings, try to throw away parts of the data -> when can we not find this anymore

\subsection{RQ4: What are the requirements for the input size?}
\todo[inline]{
benny didn't like ``requirements''
-> How much input is needed to produce 'useful' results?
How big does the system under analysis need to be?
How many type usages are necessary to \ldots
}

% motivation + general
besides the sensitivity to low quality input as discussed in the previous section, we are also interested in how much data this method needs to produce sensible results

% general
ie: how big of a codebase do we need to be able to ``learn''. When can we apply this to a project which does not rely on some well known open source framework (Android) where it is easy to gather additional data, but on for example an in-house-closed-source library oä.
extremely hard to answer especially if the findings regarding benchmark validity are more or less negative.
however, even if I cannot give experimental answers, I can at least give some lower bounds + thoughts
(need at least 10 tus within the same ``category'' to even reach a score > 0.9, from practical results in qualitative analysis, that is probably not enough, blabla)

% techniques + results

\subsection{RQ5: Can the Majority Rule also detect superfluous or wrong Method Calls?}

% motivation + intro
in chapter X we extend the method for detecting missing method calls that monp et al proposed to also detect two other anomalies:
superfluous and wrong method calls
to understand if theses variations offer anything of value we evaluate their findings manually in the same manner as we evaluate the findings of the vanilla variant.

manual eval of results when using superfluous / wrong variants
-> same mode of evaluation as RQ1

already problem I'm seeing: things with 1 call will always be almost similar since only one call is different
 -> this could be a problem?

\section{Discussion}

also mention runtime!

average number of findings per app (remember that the findings now are for all 626 apps)
how is the quality of findings (RQ1), how many findings are there, how much work is it to evaluate them

and of course regarding the ``results'' always qualify -> this only for android, open source apps, manual evaluation, blabla, so we never actually know for sure

pick a couple of high scoring findings and explain the failure modes (eg doesn't take branching into account, etc)
+ of course the REAL bug + some real smells

\todo[inline]{
mention in the end that i tried a 
Better anomaly detection (is the anomality rule actually a GOOD measure for this kind of anomalies, or should we use something totally different?))
    clustering detector try (+ hypersphere idea?)
    but all in all it didnt work out / not enough time
    + vermutungen warum es nicht funktioniert -> dataset exploration + size of method and typeusage lists per partition!

mention this when discussing the results!
    static functions evaluation!
    something to fix the dotchaining problems
    -> for both basically I'm just saying that i looked into it and it was too much work / impossible?
}

\section{Threats to Validity}
\todo[inline]{needs to be extra section or can be together with discussion?}

explain problems with the automatic evaluation
how related are the degraded TUs to missing method calls you can find in the wild?
improving the metrics by dropping cases where we know we won't find an answer

wie sehr sind die resultate aus der Android case study 1. Wahr (subjektive bewertet etc) 2. Übertragbar auf andere Anwendungsfälle(sind open-source programme of vergleichbar mit professionellen, Android eco System mit anderen, etc)
