\chapter{Evaluation}\label{ch:eval}
roter faden: results of manual analysis vs benchmark (ie. is context better than no context)

Which questions are we trying to answer with the evaluation?
    qualitative evaluation: are the findings useful in practice?
    quantitative evaluation: given some assumptions, how useful can we expect this technique to be?
    make sure I will be able to a) answer those questions and b) the answers are ``interesting''

    do the general assumptions hold? (most tus have a low score, most apps have few findings, etc)
        -> answer with graphs related to general score verteilung
	how "useful" is this verfahren -> how many true findings in relation to false positives does it find + average number of findings per app (remember that the findings now are for all 626 apps)

general idea behind evaluation method and what we are trying to detect
actually it's a sort of simulation (see recommender system book p. 301f) - micro vs macro evaluation, \ldots
    imitation of the real system of software development
    using a much simpler system, namely dropping method calls
    obvious question: how similar are the such created mmcs to mmcs in the wild?
Robustness explanation
    wie viel brauche ich um sinnvoll viel zu lernen - hälfte der leute machen fehler -> wie robust brauchst du die daten / ist das verfahren !
    what is needed for it to work (lines of code, feature richness, etc) 
    problem: how to evaluate when it is "working" vs not working -> again simulate "missing methods", compare performance?
Metrics explanation - Precision, Recall
    additional metrics from recommender systems book p.245f (robustness, learning rate - p. 261)
    performance -> training time + performance when working


checking if their ``mistakes'' / inaccuracies make a difference or not

(Case Study - if time...)
    ANDROID?!
    would be super sexy to apply the best method to some library / framework and see what happens

subsection: threats to validity!!!

qualitative evaluation:
    pick a couple of high scoring findings and explain the failure modes (eg doesn't take branching into account, etc)

explain problems with the automatic evaluation
    how related are the degraded TUs to missing method calls you can find in the wild?
    improving the metrics by dropping cases where we know we won't find an answer
    also mention runtime!
    wie sehr sind die resultate aus der Android case study 1. Wahr (subjektive bewertet etc) 2. Übertragbar auf andere Anwendungsfälle(sind open-source programme of vergleichbar mit professionellen, Android eco System mit anderen, etc)
    \ldots
