\chapter{Implementation}\label{ch:impl}

\todo[inline]{Generally: don't make this section too long I'd say!}

After laying out the theoretical foundations of detecting missing method calls using the majority rule in the last chapter, this chapter focusses on the practical side.
We give details about design decisions taken and the reasoning behind them, pitfalls that had to be overcome and the trade-offs that had to be accepted.
Additionally, we explore some of the mistakes made and dead ends that we encountered.

The first section focusses on the internal workings of the system we developed.
It explains the steps from receiving the input program to outputting a list of anomalies which hint at potentially missing method calls.
The second section revolves around the benchmark system, the basic idea behind it and how it is designed to ensure maximum flexibility.

\section{Procedure}

Our system is (primarily) a system for detecting missing method calls.
It works by statically analyzing a given software application, extracting the type usages which it contains and finally determining if any of them are anomalous by the Majority rule as described in Section \ref{sec:majority}.
The end result should be a list of locations which are potentially missing a method call.
Given a software system, which shall be tested for anomalies, conceptually the following steps have to be realized:

\begin{enumerate}
    \item Extract all type usages from the software
    \item For each type usage $x$ among them:
    \begin{enumerate}
        \item Search for type usages which are exactly similar to $x$ (i.e. calculate $E(x)$)
        \item Search for type usages which are almost similar to $x$ (that is determine $A(x)$)
        \item Calculate the strangeness score of $x$
        \item Extract the list of potentially missing calls and their likelihood $\phi$
    \end{enumerate}
    \item Output a list of anomalous type usages, sorted by their $\operatorname{S-score}$, together with the calls they are potentially missing
\end{enumerate}

\begin{figure}[h]
\centering
\begin{tikzpicture}
    \tikzset{vertex/.style={draw,rounded corners,align=center}}
    \tikzset{edge/.style = {->,> = latex'}}

    % start dot
    \node[fill=white] (startTxt) {Application};
    \node[fill=black,circle,inner sep=2pt] (start) [below = 0.25cm of startTxt] {};

    % java + db
    \node[vertex] (j1) [right = 1.5cm of start] {Read\\bytecode};
    \node[vertex] (j2) [right = 0.5cm of j1] {Extract\\type usages};
    \node[vertex] (db1) [right = 0.5cm of j2] {Persist\\type usages};

    \draw[edge] (start) to (j1);
    \draw[edge] (j1) to (j2);
    \draw[edge] (j2) to (db1);

    % py + db + end
    \node[vertex] (db2) [below = 0.25cm of db1] {Answer\\query};
    \node[vertex] (py1) [below left = 0.5cm of db2] {Request\\current set};
    \node[vertex] (py2) [below right = 0.5cm of db2] {Calculate\\ $\operatorname{S-score}$};
    \node[fill=black,circle,inner sep=2pt] (end) [right = 1.5cm of py2] {};
    \node[fill=white,align=center] (endTxt) [above = 0.25cm of end] {List of\\anomalies};

    \draw[edge] (py1) to (db2);
    \draw[edge] (db2) to (py2);
    \draw[edge] (py2) to (end);

    % big dashed boxes
    \node[label=above left:{Java}, draw=black, thick, dashed, inner sep=0.25em, fit=(j1) (j2)] {};
    \node[label=right:{Database}, draw=black, thick, dashed, inner sep=0.25em, fit=(db1) (db2)] {};
    \node[label=left:{Python}, draw=black, thick, dashed, inner sep=0.25em, fit=(py1) (py2)] {};

\end{tikzpicture}
\caption{System overview}
\label{fig:overview}
\end{figure}
\todo{make a bit nicer (alles bisi auseinander ziehen?), better position dash box labels | POTENTIALLY add the retrieve sets step to python?}
% https://tex.stackexchange.com/questions/58878/tikz-set-node-label-position-more-precisely

%-- description of system
However, this simple outline does not represent the actual realities of the system.
Instead of a singular process with [flat / not branching / \ldots] flow, it is split into three different parts which are laid out in Figure \ref{fig:overview}.
First, a java application reads the byte code of the program under analysis and iterates over all method definitions to extract the type usages which are present.
The extracted type usages are then persisted in a database to ensure flexibility and high performance in the following analysis phase.
For the actual anomaly detection we use a relatively simple python program to enable fast prototyping and again flexibility.
It goes through all sets of type usages (more on those sets in Section \ref{sec:anomaly}), requests the type usages in the set and calculates their strangeness score.
After iterating through all the sets of the application it finally outputs the results.

%summary of rest
The following sections give a more in depth explanation of the separate steps.
\todo{smth more?}

\subsection{Bytecode Analysis}\label{sec:bytecode}
\todo[inline]{maybe rename section?}

%-- general intro
We use Soot \footnote{\url{https://sable.github.io/soot/}} for the bytecode analysis.
Soot was originally a Java optimization framework but has since evolved to support a wide range of usages.
One can use it to analyse, instrument, optimize and visualize Java and Android applications.
For this purpose it provides call graph construction, points-to analysis, def or use chains, inter and intra-procedural data-flow analysis and taint analysis.
We do not need most of it's functionality, but simply use it to statically extract type usages from the input application.

%-- why bytecode over sourcecode
It would be possible to extract type usages from the source code, but extracting them from compiled code has some advantages.
The JVM byte code is very standardized and so is the Dalvik byte code found in Android applications, which facilitates the analysis.
While Soot supports source code analysis in theory, it only works up to Java 7 and byte code analysis is the recommended approach.
Besides that, using compiled applications simplifies our experimental setup and as a marginally useful side-effect, it also helps with analyzing obfuscated applications.
Finally, there are no real disadvantages to this approach, if the program is only available as source code we can simply compile it before analyzing it.

%-- some more in depth info on how soot does what it does?
Soot operates by transforming the given program into intermediate representations which can then be optimized or analysed.
It provides four intermediate representations with different use cases, we use Jimple, Soot's primary representation.
Jimple is a typed 3-address representation, which is especially suited for optimization, but also suffices for our purpose.

%-- soot startup and settings?
Soot is a very powerful framework and one can customize it to carter quite specialized requirements.
We will mostly analyze Android applications and the settings we are using reflect this.
In Figure \ref{fig:sootparam} we have listed the most important settings we are using to configure Soot.
\todo[inline]{more detailed description of settings? i think not}

\begin{figure}[h]
    \centering
    \begin{tabular}[h]{c|c|c}
    Option & Parameter & Explanation \\ \hline
    \code{-app } & - & Run in application mode \\ \hline
    \code{-keep-line-number} & - & Keep line number tables \\ \hline
    \code{-output-format} & none & Set output format for Soot \\ \hline
    \code{-allow-phantom-refs } & - & Allow unresolved classes \\ \hline
    \code{-src-prec} & apk-class-jimple & Sets source precedence to format files \\ \hline
    \code{-process-multiple-dex} & - & Process all DEX files found in APK \\ \hline
    \code{-android-jars} & [path] & The path for finding the android.jar file \\ \hline
    \end{tabular}
    \label{fig:sootparam}
    \caption{The parameters passed to Soot}
\end{figure}

\subsection{Extracting Type Usages}

\todo[inline]{
some more on extraction! + the algorithm!
some information on what kind of further analysis is attempted (local must alias, bla)
general / not sure where yet: 
explain the changes I made to their code ie proper refactoring, etc
}

%-- general ablauf
The Soot execution is divided into a number of 'phases' each of which has a specific task like for example the aggregation of local variables.
The actual work is done by so called transformations, which are able to modify the input they receive but are not required to do so.
The phases are further grouped into 'packs' and the registered packs are applied successively during the execution.
We register a custom piece of code in the Jimple transformation pack, which is applied to every method in the analysed program.

%--- what is the transformer doing?
This custom transformation receives as input a Jimple representation of the method body it is currently analyzing.
To extract the type usages, it iterates through all the statements in the method and marks those which are invoking a method.
In the end it has extracted a list of all method calls that are happening in this method.
It now iterates through this list of method calls and groups them into type usages.
To do so, it first checks on which variable or object the current call was invoked and checks if there is already a type usage for this object.
If there is, it adds the call to the type usage and advances to the next method call in the list.
If there is no type usage for the object so far, it creates a new one.
After completing this process for all method invocations in the method body the analysis is complete and it returns the list of type usages it extracted.

%-- local must alias analysis
One important step of this analysis is the 'LocalMustAliasAnalysis', which attempts to determine if two local variables (at potentially different program points) must point to the same object.
This is necessary to decide if the calls made on these locals should be grouped into one type usage.
The underlying abstraction is based on global variable numbering and follows the ideas presented by Lapowsky et al. \cite{lapkowski1998extended} with some minor adaptions.
This analysis is a Soot feature and in test runs on big applications we noticed that it can be very expensive.
Because of this we made it optional, however for our benchmarks and evaluation we were able to use it.

%-- ignoring some classes
One additional detail is that we are excluding some classes from the Android analysis for performance reasons.
The packages we exclude in this manner are \code{java.*}, \code{android.*}, \code{soot.*} and  \code{javax.*}.
These are all framework classes and we are primarily interested in the type usages occurring in the application itself not in the framework.
This means that we do not store type usages that occur \emph{inside} of those classes, we do, however, still record usages of them.
\todo{explain a bit better that these classes are kind of exactly what we are most interested in as far as type usages go, but we don't want too step ITNO them}

\subsection{Storing Type Usages}

%intro / übergang + general info
After the transformer has analyzed the current method body and extracted a list of type usages from it, we persist them into a database.
We are using HSQLDB\footnote{\url{http://hsqldb.org/}}, a SQL relational database written in Java.
It provides a small, multithreaded and transactional database engine with in memory or disk based tables.
We chose HSQLDB because it is small and has good performance as well as an easy setup.
Additional arguments where its permissive license and the large number of features it supports.
We also invested some time into exploring the possibility of using PostgreSQL\footnote{\url{https://www.postgresql.org/}}, but as explained in Section \ref{sec:deadends} it turned out that its performance was inferior.

%- why are we using a database at all?
In their work, Monperrus et al. save all data in a text based format, but this means they need to parse everything again for the analysis, which can be slow and require a lot of memory for large inputs.
Additionally, the data takes up a lot of storage space when persisted to disk.
In contrast, using a database has many positive ramifications.
First off, retrieving data from a database is fast and we can access the data selectively based on changing criteria.
Additionally, we gain a lot of flexibility and it becomes easier to extend both, the stored data and the analysis itself.
This enabled us to quickly build several prototypes of the analysis using Python.

% main advantage: big db possible
% ---- CONTINUE HERE!!!
Finally, storing the type usages in a database is the first step for eventually building a large dataset of type usages.
While it can be useful to analyse only those type usages which occur in one application, 


somewhere(here?) mention, that it is not only possible but even SMART to not only incorporate the type usages of ONE application into the analysis but if at all possible, all of the software which uses the same kind of framework/library
+ explain why obviously
    of course it's useful to ALSO analyse tus over classes which are private to my application (if there is enough data)
    however, mostly interesting are the framework classes which are used by many applications
    when analyzing the usages of framework etc classes, we want to have as much information as possible

%-- database layout + some of the considerations made (x. normalenform, sowas?)
-> some sort of sql graphic? -> use the one I get from Intellij!
+ describe obviously (+ what was made how why)

\subsection{Anomaly Detection}\label{sec:anomaly}
only part affected by the different variants presented in chapter 3
retrieving relevant type usages for the current variant + potentially modifying them + throwing them into the detector
-> explain a bit the python class structure I developed?

maybe another img or flow chart for python ablauf (or too much detail?)o

somewhere explain the ``sets of tus'' + peformance issues before (ref db dead end)

\todo[inline]{mention that monperrus hold 0.9 as treshhold for smth being an anomaly and that this will be our cutoff oä}

\subsection{Improvements and Dead Ends}\label{sec:deadends}
\todo{should this be a section or a subsection?}

explaining all the work i did + what was already there
    first reading their code, coming across a couple of discrepancies between code and paper, later check if everthing still works (evaluation)
    refactoring everything + saving stuff to database
    building python infrastructure for analysis

evaluation of other db system + why it didn't work (basically pure db stupidity)

some changes that had to be made to the analysis framework for android analysis

why some solutions where discarded (eg pure database / could be revisited if it turns out to be the best anyways - performance)
Better anomaly detection (is the anomality rule actually a GOOD measure for this kind of anomalies, or should we use something totally different?))
    clustering detector try (+ hypersphere idea?)

static functions evaluation!
something to fix the dotchaining problems

\section{Benchmark}
some more subsections? (different ways for degrading type usages?)

this only to be about the automatic benchmark stuff!

building benchmarking infrastructure
flexibility with python class setup
