\chapter{Related Work}
%todo Position this here or just before conclusion? - right now here seems better though

%--- relevance of code smells and finding them
finding bugs in code is hard
removing code smells and irregularities can contribute to code quality and thus make finding bugs easier / prevent them
especially for larger software systems, automatically detecting low quality code and improving it can contribute significantly to the maintainability of the code
Thus, there has been a lot of interest in automatic approaches for detecting code smells (and bugs)

%--- section summary
In this section we will present a number of different approaches for smell detection.
We start with some ``conventional'' methods which mostly rely on hard coded rules and pattern matching (?)
Give a quick overview of Smell detection specificly related to android applications, as we are applying the method studied in this thesis to a number of android apps in the evaluation section
Finally we look into techniques which attempt to learn rules and properties from the code they are analyzing instead of relying on rules which were given bei the developer a-priori.

\section{Finding Code Smells with static Analysis}
%todo section title like this or smth different? (hard coded rules, ..)

Findbugs (url) is a static analysis tool that finds bugs / smells in Java code
reports around 300 bug patterns + potentially more using a plugin architecture
specific detector for each of the patterns, with special sometimes complex detection mechanism (hand crafted!)
detectors often built starting from real bugs (attempting to find the bug in question and all similar ones automatically)

Ayewah et al. \cite{ayewah2007evaluating} apply it to several large open source applications (Sun's JDK and Glassfish J2EE server) and portions of googles java codebase
premise: static analysis often finds true but trivial bugs, in the sense that these bugs won't really cause a defect in the software / are intentional / don't cause any measurable misbehavior
examples are deliberate errors, infeasible situations (``cannot happen'') or situations which cannot be recovered from anyways
in their analysis they find 379 medium and high priority warnings which they classify as follows
5 due to erroneous analysis by findbugs
160 as impossible or having little to no functional impact
176 as potentially having some impact
38 as true defects having substantial impact (the real behavior is clearly not as intended)
takeaway: this can find a lot of true bugs, but also lots of false positives and it can be difficult to distinguish them

To alevieate the problem of high false positives among the findings reported by findbugs
Shen et al. \cite{shen2011efindbugs} propose a ranking method which attempts to rank true bugs before less important warnings
based on defect likelihood -> manual analysis of findings of a large project (JDK) and flagged as true / false finding for each
then across bug categories and types calculate likelihood of a finding being a true finding (from the manual analysis data)
Findings with higher likelyhood are displayed first (with variance as tiebreaker)
additionally the rankings are updated using user feedback on their projects
Evaluation with 3 open source applications (Tomcat, AspectJ, Axis)
comparing their ranking against the default severity ranking of findbugs (basically hardcoded value for each bug type)
using a cutoff at 10, 20, 30, \ldots percent of the total findings
-> precision and recall systematically better than the default ranking, especially for cutoffs around 50\%

%todo Which other papers?!

\section{Android specific Smell Detection}
%todo move this section up to after the general smell detection section?

In the evaluation section \ref{ch:eval} we are analyzing a large number of open source android applications.
Thus, as a small overview of android related code smell detection consider the following papers.

quick mention of the PAPRIKA paper mentioned in aDoctor intro? \cite{hecht2015detecting}
    operates on byte code, but tries to infer smells on the source code level
    builds graph model of the application from byte code and stores it in graph db (nodes are entities such as the app itself, classes, methods, attributes and variables, further annotated with specific metrics)
    uses cypher query language to detect the smells -> basically hand coded rules as well
    only recognizes 4 android specific smells (and 4 general ones)
    Evaluation: specifically developed wittness application whith 62 known smells
    using the right kind of metrics they whereable to reach precision and recall of 1 on this witness application
    further they apply their tool to a bunch of free android apps in the playstore and make some assertions about the prelevance of antipatterns in publicly available applications
    findings: some antipatterns, especially related to memory leaks appear in up to 39\% of applications, ``big'' ones like facebook or skype

After Reimann et al. \cite{reimann2014tool} presented a catalogue of 30 android specific code smells, Palomba et al. \cite{palomba2017lightweight} developed a tool to detect 15 of them.
It works on the abstract syntax tree of the source code and uses specifically coded rules to detect each of the smells.
For the evaluation of their approach they examine 18 Android applications by comparing the results of their tool against a manually built oracle.
The oracle was created by having two Masters students study the code of the applications in depth and manually flag offending locations.
While they reach an average precision and recall of 98\%, there are a number of cases in which their tool fails or yields false positives.
These cases are exactly those where the hard coded detection rules do not consider a special case, a new library or otherwise changing criteria.

[explain shortcoming a bit better!]
example where they only included 2 compression libraries in their rules, but there is a third new one

\section{Inferring Properties from the Code at Hand}

As the results of the previous paper showed, it can be difficult and costly to keep hard coded detection rules up to date and relevant.
Because of this, there has been a lot of interest in approaches which adapt automatically to changing requirements by learning rules from the source code and looking for violations of those rules.

[Why is this potentially better than other approaches (actually learning an api vs static rules comes to mind),
also mention some stuff about recommender systems in general (it is the official thesis topic after all\ldots)]

Mention \cite{engler2001bugs} as probably the first paper which proposed the general idea behind DMMC -> learning from the code at hand, instead of using static predefined rules

General: read the summary paper \cite{robillard2013automated} again and check which approaches are there and which could / should be mentioned
+ check to read stuff\ldots

Remember the general machine learning approaches which were not super successful, but at least learned a LOT
especially like general code smells for example

\section{Previous work on detecting missing method calls / object usages}
-> api missuse detection

there have been a number of works concerned with finding patterns in object usages and using those to find potential bugs

short summaries of the two DMMC papers: \cite{monperrus2010detecting} and \cite{monperrus2013detecting}
most important for this works are the two papers by Monperrus et all (cite)
which introduced the notion of almost similarity and are the primary inspiration for this work
they consider the invocation of methods on an object of a particular type in a particular context a type usage
here the context is nothing more than the name of the method in which the object is used and its signature (type of its parameters)
after mining a list of all the type usages present in a code base, they relate the number of exactly equal type usages to the number of almost equal ones
exactly equal means that context, type and method list are identical and almost equal means the same only that the method list can contain one additional method
if there are a lot of almost equal type usages and very few equal ones, the tu under scrunity is probably an anomaly.
more details on their method can be found in the next section

results of monperrus et all:
% TODO do results!

Before this, Wasylkowski et al. \cite{wasylkowski2007detecting} introduced a method to locate anomalies in the order of methodcalls.
First, they extract usage models from Java code by building a finite state automata for each method.
The automata can be imagined similarly to the control flow graph of the method with instructions as transitions in the graph.
From these they mine temporal properties, which describe if a method $A$ can appear before another method $B$.
One can imagine this process as determining if there exists a path through the automata on which $A$ appears before $B$, which in turn implies an call to $A$ can happen before one to $B$.
Finally, they are using frequent itemset mining \cite{han2006data} to combine the temporal properties into patterns.
% todo add short explanation of frequent itemset mining? I think not really relevant

In this work an anomaly also occurs when many methods respect a pattern and only a few (a single one) break it.
In their experiments they find 790 violations when analyzing an open source program and find 790 violations.
Manual evaluation classifies those into 2 real defect, 5 smells and 84 ``hints'' (readability or maintainability could be improved).
This adds up to a fals positive rate of 87.8\%, but with an additional ranking method they were able to obtain the 2 defects and 3 out of 5 smells within the top 10 results.

In a related work Nguyen et al \cite{nguyen2009graph} use a graph-based representation of object usages to detect temporal dependencies.
This method stands out because it enables detecting dependencies between multiple objects and not just one.
% todo what does the branching thing even mean?
The object usages are represented as a labeled directed graph where the nodes are field accesses, constructor or method calls and branching is represented by control structures.
The edges of the graph represent the temporal usage order of methods and the dependencies between them.
% todo is this merge sort thing true?!
Patterns are then mined using a frequent induced subgraph detection algorithm which builds larger patterns from small patterns from the ground up, similar to the way merge sort operates.
Here an anomaly is also classifed as a ``rare'' violation of a pattern, i.e. it does not appear often in the dataset in relation to its size.
In an evaluation case study this work finds 64 defects in 9 open source software systems which the authors classifies to 5 true defects, 8 smells and 11 hints, which equals a false positive rate of 62.5\%.
Using a ranking method the top 10 results contain 3 defects, 2 smells and 1 hint.

% todo mention more related work from detecting mmc paper?

% \section{Section on Anomaly Detection}

